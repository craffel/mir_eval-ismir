% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{braket}
%\usepackage{authblk}
\usepackage{booktabs}

\usepackage{microtype}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}
\DeclareMathOperator*{\median}{median}
\newcommand{\ind}[1]{\ensuremath{\left\llbracket#1\right\rrbracket}}
\def\given{\ensuremath{|~}}
\def\defeq{\ensuremath{:=}}

\def\mireval{\texttt{mir\char`_eval}}

% ------
\title{\mireval{}}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%\author[1]{Colin Raffel}
%\author[1]{Brian McFee}
%\author[2]{Justin Salamon}
%\author[2]{Eric J. Humphrey}
%\author[2]{Oriol Nieto}
%\author[1]{Dawen Liang}
%\author[1]{Daniel P. W. Ellis}
%\affil[1]{LabROSA, Dept. of Electrical Engineering\\Columbia University, New York}
%\affil[2]{Music and Audio Research Lab\\New York University, New York}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Central to the field of MIR research is the evaluation of algorithms used to extract information from music data.
We present \mireval{}, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms.
In the present work, we enumerate the metrics implemented by \mireval{} and quantitatively compare each to an existing implementation.
When the score reported by \mireval{} differs substantially from the reference, we detail the differences in implementation.
We also provide a brief overview of \mireval{}'s architecture, design, and intended use.
\end{abstract}
%
\section{Evaluating MIR Algorithms}

Many tasks in the field of Music Information Retrieval (MIR) involve the development of systems which can automatically produce semantic information about a piece of music.
The effectiveness of such a system is typically judged based on how similar it acts to a human \cite{downie2003toward}.
A straightforward way to make this judgement would be for a human to study the output produced by the system and judge its correctness.
However, this process is inherently subjective and also extremely time-consuming when evaluating a system's output over a large corpus of music.

As a result, metrics have been defined for each task which provide a well-defined way of computing a score which denotes the correctness of a system's output.
These metrics typically involve a heuristically-motivated comparison of the system's output to a reference which is known to be correct.
Over time, certain metrics have become standard for each task, so that the performance of systems created by different researchers can be compared when they are evaluated over the same dataset \cite{downie2003toward}.
Unfortunately, this comparison can be confounded by the fact that the actual implementation of the metric can affect the scores it produces.

% MIREX is the standard, but it's not community-developed, not transparent, not well-documented, uses multiple languages, is coupled to NEMA, not easy to use, dependencies, not used outside of MIREX, etc.
For the past 10 years, the yearly Music Information Retrieval EXchange (MIREX) has provided setting in which different MIR algorithms may be compared against common datasets \cite{downie2008music}.
By providing a standardized shared-task setting to track progress, MIREX has proven to be critically useful in the evolution of MIR research.
MIREX relies on the Networked Environment for Music Analysis (NEMA) \cite{west2010networked}, a large-scale system which includes exhaustive functionality for evaluating, summarizing, and displaying evaluation results.
The NEMA codebase includes multiple programming languages and dependencies (including proprietary ones, e.g. MATLAB) so compiling and running it for research uses is nontrivial.

Due to its scale, characteristics, and intended use, the NEMA system is rarely used for evaluating MIR algorithms outside of the setting of MIREX \cite{downie2008music}.
As a result, researchers typically create their own implementations of common metrics for evaluating their algorithms.
These implementations are not standardized, and even worse, may contain bugs.
Both of these factors can confound comparison of different systems.

These factors motivate the development of a standardized software package which implements the most common metrics used to evaluate MIR systems.
Such a package should be straightforward to use and well-documented so that it can be easily adopted and understood by MIR researchers.
In addition, it should be community-developed and transparently implemented so that all design decisions are well-reasoned and easily understood.

Motivated by these criteria, we present \mireval{}, a software package which intends to provide an easy and standardized way to evaluate MIR systems.
The present work first discusses the architecture and design of \mireval{} in Section \ref{sec:architecture}.
In Section \ref{sec:tasks}, we describe all of the tasks covered by \mireval{} and the metrics included.
In order to validate our implementation decisions, we compare \mireval{} to existing software in Section \ref{sec:comparison}.
Finally, we discuss and summarize our contributions in Section \ref{sec:discussion}.

\section{\mireval{}'s architecture}
\label{sec:architecture}

% Task submodules, one function per metric
% io submodule for loading in data
% Evaluators

\section{Tasks included in \mireval{}}
\label{sec:tasks}

\subsection{Beat Detection}

The aim of a beat detection algorithm is to report the times at which a typical human listener might tap their foot to a piece of music.
As a result, most metrics for evaluating the performance of beat tracking systems involve computing the error between the estimated beat times and some reference list of beat locations.
Many metrics additionally compare the beat sequences at different metric levels in order to deal with the ambiguity of tempo \cite{levy2011improving}.

\mireval{} includes the following metrics for beat tracking, which are defined in detail in \cite{davies2009evaluation}:
The \textbf{F-measure} of the beat sequence, where an estimated beat is considered correct if it is sufficiently close to a reference beat;
\textbf{Cemgil's score}, which computes the sum of Gaussian errors for each beat;
\textbf{Goto's score}, a binary score which is 1 when some specific heuristic criteria are met;
\textbf{McKinney's P-score}, which computes the cross-correlation of the estimated and reference beat sequences represented as impulse trains;
\textbf{continuity-based scores} which compute the proportion of the beat sequence which is continuously correct;
and finally the \textbf{information gain} of the beat error histogram to a uniform distribution.

\subsection{Chord Recognition}

% relevant papers
% - pauwels & peeters
% - matthais
% - harte
% - mirex 
% - Utrecht agreement

Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord recognition is an ongoing topic of discussion.
%For example, ISMIR 2010 saw the fondly dubbed ``Utrecht Agreement on Chord Evaluation''\footnote{http://www.music-ir.org/mirex/wiki/The\_Utrecht\_Agreement\_on\_Chord\_Evaluation}, in which interested researchers met to discuss sane ways of quantifying the performance of automatic chord recognition systems.
Several recent articles address issues and concerns with vocabularies, comparison semantics, and other lexicographical challenges unique to chord recognition \cite{}.
Ultimately, the source of this difficulty stems from the inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation \cite{McVicar}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, e.g Major-Minor, Sevenths, with or without inversions, and so on.

Thanks to the previous efforts of Harte \cite{}, text representations of chord labels adhere to a standardized format, consisting of a root, quality, extensions, and a bass note; of these, only the root is strictly required.
However, in order to efficiently compare chords in a variety of different ways, it is helpful to first translate a given chord label $\mathcal{C}$ into a numerical representation, shown in Figure \ref{}.
In this example, a $G:7(9)/5$ is mapped to split into 4 pieces of information: one, the root is mapped to an absolute pitch class $\mathcal{R}$, in $[0, 11]$, where $C\to0$, $C\sharp/D\flat\to1$, etc; two, the quality is mapped to a root-invariant 12-dimensional bit vector $\mathcal{Q}$ by setting the scale degrees of the quality; three, any extensions are applied (via addition or omission) to the quality bit vector as scale degrees in a single octave, resulting in pitch vector $\mathcal{P}$; and four, the bass interval (5) is translated to the relative scale degree in semitones $\mathcal{B}$.
Note that the add-9 is rolled into a single octave as an add-2.
This is a matter of convenience as extended chords (9's, 11's or 13's) are traditionally resolved to a single-octave equivalent, but the bit-vector representation could be easily expanded to represent such information.

Having gone through this bit of effort, it is now straightforward to compare chords along the five rules used in MIREX 2013:
\begin{enumerate}
\item Root:
	\begin{enumerate}
	\item $\mathcal{R}_{est} == \mathcal{R}_{ref}$
	\item $\forall \mathcal{Q}_{ref}$
	\end{enumerate}
\item Major-Minor: Rule 1.a, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref}$
	\item $\mathcal{Q}_{ref} \in \{Maj, min\}$ 
	\end{enumerate}
\item Major-Minor w/Inversions: Rule 2, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$
	\end{enumerate}
\item Sevenths: Rule 1.1, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref} $
	\item $ \mathcal{Q}_{ref} \in \{Maj, min, Maj7, min7, 7\}$ 
	\end{enumerate}
\item Sevenths w/Inversions: Rule 4, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$ 
	\end{enumerate}
\end{enumerate}

Following recent trends in MIREX, an overall score is computed by weighting each comparison by the duration of its interval, over all intervals; stated another way, this is the piecewise continuous-time integral of the intersection of two chord sequences, $(\mathbf{C}_{ref}, \mathbf{C}_{est})$, expressed as follows:

\begin{equation}
S(\mathbf{C}_{ref}, \mathbf{C}_{est}) = \frac{1}{T}\int_{t=0}^{T} \mathcal{C}_{ref}(t) == \mathcal{C}_{est}(t)
\end{equation}

\noindent Here, this is achieved here by forming the union of the boundaries in each sequence, and summing the time intervals of the correct ranges. Note that equivalence is subject to one of the rules defined previously. 

Finally, the total score over a set of $N$ items is given by a discrete summation, where the importance of each score, $S_n$, is weighted by the duration, $T_n$, of each annotation:

\begin{equation}
S_{total} = \frac{\sum_{n=0}^{N} T_n*S_n}{\sum_{n=0}^{N} T_n}
\end{equation}
 
\subsection{Pattern Discovery}

This task aims to evaluate algorithms that identify musical patterns (i.e. short fragments or melodic ideas that repeat at least twice) both from audio and symbolic representations.
Given the novelty of this task \footnote{The first year to appear on MIREX was 2013.}, the evaluation metrics are likely to be modified in further editions.
Nonetheless, Collins put together all the previously existent metrics and a few new ones for this task in its first appearance in MIREX\cite{Collins2013}, which resulted in 19 different scores, each one (except for the two that evaluate the algorithm execution time) implemented in MIR-eval and described below:

\begin{itemize}
    \item
      \textbf{Standard F-measure, Precision, and Recall (F$_1$, P, R)}: This metric, composed of three scores, checks if the prototype patterns of the reference match possible key-transposed patterns in the prototype patterns of the estimations.
      Since the sizes of these prototypes must be equal, this metric is quite restrictive and it tends to be 0 most of the time (see 2013 MIREX results).
    \item
      \textbf{Establishment F-measure, Precision, and Recall (F1$_{est}$, P$_{est}$, R$_{est}$)}: These scores evaluate the amount of patterns that were successfully identified by the estimated results, no matter how many occurrences they found.
      In other words, this metric captures the ability of the algorithm to \textit{establish} that the estimated patterns are actually contained in the reference annotation.
    \item 
      \textbf{Occurrence F-measure, Precision, and Recall (F1$_{occ}$, P$_{occ}$, R$_{occ}$)}: Independently of how many patterns were correctly established, we may also want to know how well the algorithm finds all their respective occurrences throughout the piece.
      This metric aims to quantize this \textit{occurrence} retrieval quality of the algorithm.
      This metric has a tolerance parameter $c$ to allow differences when evaluating the similarity between occurrences, and in MIREX they use $c=.75$ and $c=.5$.
    \item
      \textbf{Three-layer F-measure, Precision, and Recall (TLF$_1$, P$_3$, R$_3$)}: These scores can be seen as an improved version of the standard metrics.
      They capture both the establishment of the patterns and the occurrence retrieval in a single set of scores, being more permissive than the standard evaluation.
    \item
      \textbf{First $N$ patterns metrics (FFTP$_{est}$, FFP)}: This includes the first $N$ patterns target proportion establishment recall, and the first $N$ patterns three-layer precision. 
      By analyzing the first $N$ patterns only, we evaluate the ability of the algorithm of sorting the identified patterns based on their relevance. 
      In MIREX, $N=5$.

\end{itemize}

\subsection{Segmentation}

Evaluation criteria for segmentation fall into two categories: boundary annotation, and structural annotation.
\emph{Boundary annotation} is the task of predicting the times at which structural changes occur, such as when a \emph{verse} transitions to a \emph{refrain}.
\emph{Structural annotation} is the task of assigning labels to detected segments.  
The estimated labels may be arbitrary strings --- such as $A$, $B$, $C$, \etc{} --- and they need not describe functional concepts.

In both tasks, we assume that annotations express a partitioning of the track 
into intervals ${(s_i, t_i)}_{i=1}^m$, where $s_0=0$ denotes the beginning of track, 
$t_m$ denotes the end of the track, and $t_i = s_{i+1}$.
Structural annotation additionally requires that each interval be assigned a label $y_i$.

\mireval{} implements the following MIREX-compatible boundary detection metrics:
\begin{description}
\item[Boundary detection:] precision, recall, and F-measure for detecting boundary
events within a tolerance window $w\in \{0.5, 3\}$~\cite{turnbull2007supervised};
\item[Boundary deviation:] median absolute time difference from a reference boundary
to its nearest estimated boundary, and vice versa~\cite{turnbull2007supervised},
\end{description}
and the following structure annotation metrics
\begin{description}
\item[Pairwise classification:] precision, recall, and F-measure for classifying pairs
of sampled time instants as belonging to the same structural
component~\cite{levy2008structural};
\item[Rand index:\footnote{The MIREX results page refers to Rand index as ``random
clustering index''.}] reference and estimated annotations are sampled uniformly
throughout the track, and the induced clusterings are compared by the Rand index
~\cite{rand1971objective};
\item[Normalized conditional entropy:] reference and estimated labels are sampled
uniformly, and interpreted as samples of random variables $Y_R, Y_E$, which are
computed by estimating the conditional entropy of $Y_R$ given $Y_E$
(\emph{under-segmentation}) and $Y_E$ given $Y_R$ 
(\emph{over-segmentation})~\cite{lukashevich2008towards}.
\end{description}

% \subsubsection{Boundary annotation}

% Within boundary annotation, there are two categories of evaluation metrics: detection,
% and deviation~\cite{turnbull2007supervised}.  

% \emph{Boundary detection} measures the precision, recall, and F-measure of boundary prediction within a window.
% Let $B^R$ ($B^E$) denote the set of unique interval boundaries in the reference (estimated) annotation, and let $W$ denote 
% a window, typically either $0.5$s or $3.0$s.  
% A \emph{hit} is defined as a pair $b_e \in B^E$, $b_r \in B^R$ such that $|b_e - b_r|
% \leq W$.  

% No estimated boundary is counted as a hit for more than one reference boundary, and vice versa.
% This is accomplished by computing a maximum bipartite matching $H_W$ between $B^E$ and $B^R$ (subject to the window constraint $W$)
% using the Hopcroft-Karp algorithm~\cite{hopcroft1973n}.  This deviates from the greedy heuristic used in the MIREX
% implementation, but it is guaranteed to produce a correct matching.  Precision and recall at window $W$ are defined as
% \begin{align}
% P_W(B^R, B^E) &\defeq \frac{|H_W|}{|B^E|}\\
% R_W(B^R, B^E) &\defeq \frac{|H_W|}{|B^R|},
% \end{align}
% and the corresponding F-measure is defined by taking their harmonic mean.

% \emph{Boundary deviation} is comprised of two scores, which measure the median time between a reference boundary and its nearest corresponding estimated boundary, and vice versa.
% These two metrics have previously been defined as \emph{true-to-predicted} (T-to-P) and \emph{predicted-to-true} (P-to-T), though in the terminology of this document, we use 
% alternative naming of \emph{reference-to-estimated} (R-to-E) and \emph{estimated-to-reference} (E-to-R):
% \begin{align}
% \text{R-to-E}(B^R, B^E) &\defeq \displaystyle\median_{b_r \in B^R} \min_{b_e \in B^E} \left|b_r - b_e\right|\\
% \text{E-to-R}(B^R, B^E) &\defeq \displaystyle\median_{b_e \in B^E} \min_{b_r \in B^R} \left|b_r - b_e\right|.
% \end{align}


% \subsubsection{Structural annotation}
% Two standard methods of evaluating structural annotation accuracy are \emph{pairwise classification}~\cite{levy2008structural}
% and conditional entropy~\cite{lukashevich2008towards}.  In both methods, a collection of samples are generated by sampling the
% labels at time steps between $0$ and the track duration.  The $i$th sample is assigned a reference label $y_i^R$ and
% and estimated label $y_i^E$.  Our implementation follows the MIREX guidelines, and generates samples at a default rate of 10Hz.

% Reference and estimated annotations must span identical time durations for annotation metrics to be well-defined.
% This is accomplished in our implementation by trimming or padding the estimated annotation to exactly match the start and end-times
% reported in the reference annotation, and synthesizing unique labels if necessary.

% Given the labels for the samples $\{(y_i^R, y_i^E)\}_{i=1}^n$, the \emph{pairwise classification} metrics are defined as precision,
% recall, and F-measure of label agreement over all unique, distinct pairs $i \neq j$.  
% Let $A_R = \{ \{i, j\} \given y_i^R = y_j^R\}$ denote the set of similarly labeled frames in the reference, with $A_E$ defined
% analogously for the estimation.  Precision and recall are defined as
% \begin{align}
% P_\text{pair} &\defeq \frac{ |A_E \cap A_R| }{ |A_E| }\\
% R_\text{pair} &\defeq \frac{ |A_E \cap A_R| }{ |A_R| }.
% \end{align}

% \emph{Normalized conditional entropy} scores are computed by estimating the conditional entropy of the estimated label $y^E$ given
% the reference label $y^R$ and vice versa.  Let $P$ denote the empirical joint distribution over reference and estimated labels:
% \begin{align}
% P_{ij} &\propto \left|\{t \given y_t^R = i \wedge y_t^E = j\}\right|.
% \end{align}
% Let $Y^R$ and $Y^E$ denote random variables corresponding to the reference and estimated label for an arbitrary sample.
% The conditional entropies $H\left(Y^E \given Y^R\right)$ and $H\left(Y^R \given Y^E\right)$ are estimated from the 
% empirical marginals ${p_i^R = \sum_j P_{ij}}$ and ${p_j^E = \sum_i P_{ij}}$ as described by 
% Lukashevich~\cite{lukashevich2008towards}.

% The final scores are defined as \emph{over-segmentation} ($S_\text{O}$) and \emph{under-segmentation} ($S_\text{U}$):
% \begin{align}
% S_\text{O} &\defeq 1 - \frac{H(E \given R)}{\log_2 |\ell^E|}\\
% S_\text{U} &\defeq 1 - \frac{H(R \given E)}{\log_2 |\ell^R|},
% \end{align}
% where $\ell^R$ and $\ell^E$ denote the sets of unique label values given in the reference and estimation.

\subsection{Melody Extraction}
\subsubsection{Task definition}
Melody extraction algorithms aim to produce a sequence of frequency values
corresponding to the pitch of the dominant melody from a musical recording
\cite{salamon:MelodyReview:IEEESPM13}. The estimated pitch is represented as a
time series of fundamental frequency ($f_0$) values in Hz sampled on a fixed
time grid (e.g.~every 10 ms). To evaluate the estimated sequence, a reference
sequence is generated by running a pitch tracker on the monophonic melody track
(requiring access to the multi-track recording session) and manually correcting
any mistakes made by the pitch tracker. The estimate is then evaluated against
the reference by comparing the two frequency sequences on a frame-by-frame
basis, and computing five global measures, first used in the MIREX 2005 AME
evaluation \cite{polinerMelodyEval}. The goal of these measures, defined below,
is to assess the algorithm's performance on two subtasks of melody extraction:
(1) pitch estimation, i.e.~how well the algorithm estimates the pitch of the
melody, and (2) voicing detection, i.e.~how well the algorithm determines when
the melody is present in a frame (a \textit{voiced} frame) or absent (an
\textit{unvoiced} frame). To allow evaluation of these two subtasks
independently, a melody extraction algorithm can provide a frequency estimate
even for frames it has determined to be unvoiced.

\subsubsection{Evaluation measures}
The following definitions are taken from
\cite{salamon:MelodyReview:IEEESPM13} with permission from the authors. Let the estimated melody pitch frequency
vector be $\mathbf{f}$, and the true sequence (reference) be $\mathbf{f}^*$. Let
us also define a voicing indicator vector $\mathbf{v}$, whose $\tau^\text{th}$
element $v_\tau = 1$ when a melody pitch is detected, with corresponding ground
truth $\mathbf{v}^*$.  We also define an ``unvoicing'' indicator $\bar{v}_\tau =
1 - v_\tau$.  Recall that an algorithm may report an estimated melody pitch
($f_\tau > 0$) even for times where it reports no voicing ($v_\tau = 0$).   
Then the measures are:
\begin{itemize}
  \item \textbf{Voicing Recall Rate}: The proportion of frames labeled as
  melody frames in the reference that are estimated as melody frames by the
  algorithm.
\begin{equation}
\text{Rec}_\text{vx} = \frac{\sum_\tau v_\tau v^*_\tau}{\sum_\tau v^*_\tau}
\end{equation}

  \item \textbf{Voicing False Alarm Rate}: The proportion of frames
  labeled as non-melody in the reference that are mistakenly estimated as
  melody frames by the algorithm.
\begin{equation}
\text{FA}_\text{vx} = \frac{\sum_\tau v_\tau \bar{v}^*_\tau}{\sum_\tau \bar{v}^*_\tau}
\end{equation}

  \item \textbf{Raw Pitch Accuracy}: The proportion of melody frames in the
  reference for which $f_\tau$ is considered
  correct (i.e.~within half a semitone of the ground
  truth $f^*_\tau$).
\begin{equation}
\text{Acc}_\text{pitch}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] }{\sum_\tau v^*_\tau}
\end{equation}
where $\mathcal{T}$ is a threshold function defined by:
\begin{equation}
\mathcal{T}[a] = \begin{cases}
1 \quad \text{if } |a| < 0.5 \\
0 \quad \text{if } |a| \ge 0.5
\end{cases}
\end{equation}
and $\mathcal{M}$ maps a frequency in Hertz to a melodic 
axis as a real-valued number of semitones above an arbitrary
reference frequency $f_\text{ref}$:
\begin{equation}
\mathcal{M}(f) = 12 \log_2\left( \frac{f}{f_\text{ref}} \right)
\end{equation}

  \item \textbf{Raw Chroma Accuracy}: As raw pitch accuracy, except
  that both the estimated and reference $f_0$ sequences are mapped onto a
  single octave. This gives a measure of pitch accuracy which ignores octave
  errors, a common error made by melody extraction systems:
\begin{equation}
\text{Acc}_\text{chroma}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\left<\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau)\right>_{12} \right] }{\sum_\tau v^*_\tau}
\end{equation}
Octave equivalence is achieved by taking the difference between the semitone-scale pitch 
values modulo 12 (one octave), where
\begin{equation}
\left<a\right>_{12} = a - 12 \lfloor \frac{a}{12} + 0.5 \rfloor .
\end{equation}

  \item \textbf{Overall Accuracy}: this measure combines the performance of the
  pitch estimation and voicing detection tasks to give an overall performance
  score for the system. It is defined as the proportion of all frames
   correctly estimated by the algorithm, where for non-melody
  frames this means the algorithm labeled them as non-melody, and for melody
  frames the algorithm both labeled them as melody frames and provided a correct
  $f_0$ estimate for the melody (i.e.~within half a semitone of the
  reference):

\begin{equation}
\text{Acc}_\text{ov} 
= \frac{1}{L} \sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] + \bar{v}^*_\tau \bar{v}_\tau 
\end{equation}
where $L$ is the total number of frames.
\end{itemize}

The performance of an algorithm on an entire music collection for a given
measure is obtained by averaging the per-song scores for that measure over
all songs in the collection. 

\subsubsection{Discussion}

In the measure definitions provided above, it is assumed that both the estimate
and reference sequences are sampled using the same time grid (hop size). In
practice, however, this is not always the case, since the time grid of the
reference depends on the hop size used by the pitch tracker that produced
it, and similarly the time grid of the estimate depends on the specific melody
extraction algorithm that produced it. This means that the sequences must be
resampled onto a common time-grid prior to the computation of the measures. For
the MIREX AME task, any sequence (be it reference or estimate) that is not
sampled using a 10 ms hop size is resampled using $0^{th}$-order interpolation,
i.e.~each frequency value in the target sequence is set to its nearest
neighbour (in time) from the source sequence. This kind of interpolation can
potentially have a detrimental effect on the evaluation, depending on the
difference between the source and target time grids. In particular, it can
result in artificially low scores for sequences with rapidly changing pitch
values, such as opera singing with deep vibrato. 

For this reason, the melody evaluator in \mireval{} uses $1^{st}$-order (linear)
interpolation by default in order to map the reference and estimate sequences
onto a common time-grid. Assuming the original timestamps of both sequences
correspond to the \textit{center} of each analysis frame (as they should),
using $1^{st}$-order rather than $0^{th}$-order interpolation means the results
returned by \mireval{} are lower-bounded by the MIREX results and are, in our
view, more accurate.

\subsection{Onset Detection}

The goal of an onset detection algorithm is to automatically determine when notes are played in a piece of music.  As is also done in beat tracking and segment boundary detection, the primary method used to evaluate onset detectors is to first determine which estimated onsets are ``correct'', where correctness is defined as being within a small window of a reference onset \cite{bock2012evaluating}.  From this, \textbf{precision}, \textbf{recall}, and \textbf{f-measure} scores are computed.  

\subsection{Blind Source Separation}


\section{Comparison to Existing Implementations}
\label{sec:comparison}

% Compared each mir_eval task to an existing implementation
% Parameterized the same wherever possible

% Beat: Compared against MIREX 2013
% Chord: Compared against ??
% Segmentation: Compared against ??
% Melody: Compared against MIREX 2011, SG2 only
% Pattern: Compared against development set, MATLAB code
% Source Separation: Compared against SiSEC dev sets
% Onset: Compared against MIREX 2013

% Beat - 9 metrics
% Chord - 5 metrics
% Segmentation - 14 metrics
% Melody - 5 metrics
% Pattern - 21 metrics
% Onset - 3 metrics
% Source Separation - 3 metrics

\begin{table*}[t]
  \centering
\begin{tabular}{c c c c c c c c c c}
\toprule
 \multicolumn{5}{ c }{Chord} & \multicolumn{5}{ c }{Melody}\\
  \cmidrule(lr){1-5}
  \cmidrule(lr){5-10}
 Root & M/m & M/m + Inv & 7ths & 7ths + Inv & Overall & Raw Pitch & Raw Chroma & Voicing R & Voicing F-A\\
 ??? & ??? & ??? & ??? & ??? & 0.070\% &    0.087\% &    0.114\% &    0.000\% &   10.095\% \\
  \midrule
 \multicolumn{10}{ c }{Segmentation} \\
  \cmidrule(lr){1-10}
  NCE-Over   & NCE-under  & Pairwise F & Pairwise P & Pairwise R & Rand       & F@.5       & P@.5       & R@.5       & F@3 \\
3.182\% &   11.082\% &    0.937\% &    0.942\% &    0.785\% &    0.291\% &    0.429\% &    0.088\% &    1.021\% &    0.393\% \\
  \midrule
 \multicolumn{4}{ c }{Segmentation} & \multicolumn{6}{ c }{Beat}\\
  \cmidrule(lr){1-4}
  \cmidrule(lr){5-10}
  P@3        & R@3 & Ref-est dev. & Est-ref dev. & F-Measure  & Cemgil     & Goto       &  P-score   & CMLc       & CMLt \\
  0.094\% &    0.954\% & 0.935\% &    0.000\% &  0.703\% &    0.035\% &    0.054\% &    0.877\% &    0.161\% &    0.143\% \\
  \midrule
 \multicolumn{3}{ c }{Beat} & \multicolumn{7}{ c }{Pattern}\\
  \cmidrule(lr){1-3}
  \cmidrule(lr){4-10}
  AMLc       & AMLt       & D (bits) & Pat & Pat & Pat & Pat & Pat & Pat & Pat \\
  0.137\% &    0.139\% &   10.166\% &  ??? & ??? & ??? & ??? & ??? & ??? & ???\\
  \midrule
 \multicolumn{10}{ c }{Pattern}\\
  \cmidrule(lr){1-10}
  Pat & Pat & Pat & Pat & Pat & Pat & Pat & Pat & Pat & Pat\\
  ??? & ??? & ??? & ??? & ??? & ??? & ??? & ??? & ??? & ???\\
  \midrule
 \multicolumn{4}{ c }{Pattern}  & \multicolumn{3}{ c }{Onset}  & \multicolumn{3}{ c }{Source Separation}\\
  \cmidrule(lr){1-4}
  \cmidrule(lr){5-7}
  \cmidrule(lr){8-10}
 Pat & Pat & Pat & Pat & F-measure  & Precision  & Recall & SDR & SIR & SAR \\
 ??? & ??? & ??? & ??? &    0.165\% &    0.165\% &    0.165\% & ??? & ??? & ??? \\
  \bottomrule

\end{tabular}
\end{table*}

\section{Discussion}
\label{sec:discussion}

\bibliography{mir_eval}

\end{document}
