% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}

% Title.
% ------
\title{PAPER TEMPLATE FOR ISMIR 2014}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
The abstract should be placed at the top left column and should contain about 150-200 words.
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

This template includes all the information about formatting manuscripts for the ISMIR 2014.
Please follow these guidelines to give the final proceedings a uniform look.
If you have any questions, please contact the Conference Management.
This template can be downloaded from the ISMIR 2014 web site (http://ismir2014.ismir.net).

\section{Page Size}\label{sec:page_size}

The proceedings will be printed on
 \underline{portrait A4-size paper} \underline{(21.0cm x 29.7cm)}.
All material on each page should fit within a rectangle of 17.2cm x 25.2cm,
centered on the page, beginning 2.0cm
from the top of the page and ending with 2.5cm from the bottom.
The left and right margins should be 1.9cm.
The text should be in two 8.2cm columns with a 0.8cm gutter.
All text must be in a two-column format.
Text must be fully justified.

\section{Typeset Text}\label{sec:typeset_text}

\subsection{Normal or Body Text}\label{subsec:body}

Please use a 10pt (point) Times font. Sans-serif or non-proportional fonts
can be used only for special purposes, such as distinguishing source code text.

The first paragraph in each section should not be indented, but all other paragraphs should be.

\subsection{Title and Authors}

The title is 14pt Times, bold, caps, upper case, centered.
Authors' names are omitted when submitting for double-blind reviewing.
The following is for making a camera-ready version.
Authors' names are centered.
The lead author's name is to be listed first (left-most), and the co-authors' names after.
If the addresses for all authors are the same, include the address only once, centered.
If the authors have different addresses, put the addresses, evenly spaced, under each authors' name.

\subsection{First Page Copyright Notice}

Please include the copyright notice exactly as it appears here in the lower left-hand corner of the page.
It is set in 8pt Times.

\subsection{Page Numbering, Headers and Footers}

Do not include headers, footers or page numbers in your submission.
These will be added when the publications are assembled.

\section{First Level Headings}

First level headings are in Times 10pt bold,
centered with 1 line of space above the section head, and 1/2 space below it.
For a section header immediately followed by a subsection header, the space should be merged.

\subsection{Chord Recognition}

% relevant papers
% - pauwels & peeters
% - matthais
% - harte
% - mirex 
% - Utrecht agreement

Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord recognition is an ongoing topic of discussion.
%For example, ISMIR 2010 saw the fondly dubbed ``Utrecht Agreement on Chord Evaluation''\footnote{http://www.music-ir.org/mirex/wiki/The\_Utrecht\_Agreement\_on\_Chord\_Evaluation}, in which interested researchers met to discuss sane ways of quantifying the performance of automatic chord recognition systems.
Several recent articles address issues and concerns with vocabularies, comparison semantics, and other lexicographical challenges unique to chord recognition \cite{}.
Ultimately, the source of this difficulty stems from the inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation \cite{McVicar}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, e.g Major-Minor, Sevenths, with or without inversions, and so on.

Thanks to the previous efforts of Harte \cite{}, text representations of chord labels adhere to a standardized format, consisting of a root, quality, extensions, and a bass note; of these, only the root is strictly required.
However, in order to efficiently compare chords in a variety of different ways, it is helpful to first translate a given chord label $\mathcal{C}$ into a numerical representation, shown in Figure \ref{}.
In this example, a $G:7(9)/5$ is mapped to split into 4 pieces of information: one, the root is mapped to an absolute pitch class $\mathcal{R}$, in $[0, 11]$, where $C\to0$, $C\sharp/D\flat\to1$, etc; two, the quality is mapped to a root-invariant 12-dimensional bit vector $\mathcal{Q}$ by setting the scale degrees of the quality; three, any extensions are applied (via addition or omission) to the quality bit vector as scale degrees in a single octave, resulting in pitch vector $\mathcal{P}$; and four, the bass interval (5) is translated to the relative scale degree in semitones $\mathcal{B}$.
Note that the add-9 is rolled into a single octave as an add-2.
This is a matter of convenience as extended chords (9's, 11's or 13's) are traditionally resolved to a single-octave equivalent, but the bit-vector representation could be easily expanded to represent such information.

Having gone through this bit of effort, it is now straightforward to compare chords along the five rules used in MIREX 2013:
\begin{enumerate}
\item Root:
	\begin{enumerate}
	\item $\mathcal{R}_{est} == \mathcal{R}_{ref}$
	\item $\forall \mathcal{Q}_{ref}$
	\end{enumerate}
\item Major-Minor: Rule 1.a, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref}$
	\item $\mathcal{Q}_{ref} \in \{Maj, min\}$ 
	\end{enumerate}
\item Major-Minor w/Inversions: Rule 2, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$
	\end{enumerate}
\item Sevenths: Rule 1.1, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref} $
	\item $ \mathcal{Q}_{ref} \in \{Maj, min, Maj7, min7, 7\}$ 
	\end{enumerate}
\item Sevenths w/Inversions: Rule 4, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$ 
	\end{enumerate}
\end{enumerate}

Following recent trends in MIREX, an overall score is computed by weighting each comparison by the duration of its interval, over all intervals; stated another way, this is the piecewise continuous-time integral of the intersection of two chord sequences, $(\mathbf{C}_{ref}, \mathbf{C}_{est})$, expressed as follows:

\begin{equation}
S(\mathbf{C}_{ref}, \mathbf{C}_{est}) = \frac{1}{T}\int_{t=0}^{T} \mathcal{C}_{ref}(t) == \mathcal{C}_{est}(t)
\end{equation}

\noindent Here, this is achieved here by forming the union of the boundaries in each sequence, and summing the time intervals of the correct ranges. Note that equivalence is subject to one of the rules defined previously. 

Finally, the total score over a set of $N$ items is given by a discrete summation, where the importance of each score, $S_n$, is weighted by the duration, $T_n$, of each annotation:

\begin{equation}
S_{total} = \frac{\sum_{n=0}^{N} T_n*S_n}{\sum_{n=0}^{N} T_n}
\end{equation}
 


\subsection{Melody Extraction}
\subsubsection{Task definition}
Melody extraction algorithms aim to produce a sequence of frequency values
corresponding to the pitch of the dominant melody from a musical recording
\cite{salamon:MelodyReview:IEEESPM13}. The estimated pitch is represented as a
time series of fundamental frequency ($f_0$) values in Hz sampled on a fixed
time grid (e.g.~every 10 ms). To evaluate the estimated sequence, a reference
sequence is generated by running a pitch tracker on the monophonic melody track
(requiring access to the multi-track recording session) and manually correcting
any mistakes made by the pitch tracker. The estimate is then evaluated against
the reference by comparing the two frequency sequences on a frame-by-frame
basis, and computing five global measures, first used in the MIREX 2005 AME
evaluation \cite{polinerMelodyEval}. The goal of these measures, defined below,
is to assess the algorithm's performance on two subtasks of melody extraction:
(1) pitch estimation, i.e.~how well the algorithm estimates the pitch of the
melody, and (2) voicing detection, i.e.~how well the algorithm determines when
the melody is present in a frame (a \textit{voiced} frame) or absent (an
\textit{unvoiced} frame). To allow evaluation of these two subtasks
independently, a melody extraction algorithm can provide a frequency estimate
even for frames it has determined to be unvoiced.

\subsubsection{Evaluation measures}
The following definitions are taken from
\cite{salamon:MelodyReview:IEEESPM13} with permission from the authors. Let the estimated melody pitch frequency
vector be $\mathbf{f}$, and the true sequence (reference) be $\mathbf{f}^*$. Let
us also define a voicing indicator vector $\mathbf{v}$, whose $\tau^\text{th}$
element $v_\tau = 1$ when a melody pitch is detected, with corresponding ground
truth $\mathbf{v}^*$.  We also define an ``unvoicing'' indicator $\bar{v}_\tau =
1 - v_\tau$.  Recall that an algorithm may report an estimated melody pitch
($f_\tau > 0$) even for times where it reports no voicing ($v_\tau = 0$).   
Then the measures are:
\begin{itemize}
  \item \textbf{Voicing Recall Rate}: The proportion of frames labeled as
  melody frames in the reference that are estimated as melody frames by the
  algorithm.
\begin{equation}
\text{Rec}_\text{vx} = \frac{\sum_\tau v_\tau v^*_\tau}{\sum_\tau v^*_\tau}
\end{equation}

  \item \textbf{Voicing False Alarm Rate}: The proportion of frames
  labeled as non-melody in the reference that are mistakenly estimated as
  melody frames by the algorithm.
\begin{equation}
\text{FA}_\text{vx} = \frac{\sum_\tau v_\tau \bar{v}^*_\tau}{\sum_\tau \bar{v}^*_\tau}
\end{equation}

  \item \textbf{Raw Pitch Accuracy}: The proportion of melody frames in the
  reference for which $f_\tau$ is considered
  correct (i.e.~within half a semitone of the ground
  truth $f^*_\tau$).
\begin{equation}
\text{Acc}_\text{pitch}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] }{\sum_\tau v^*_\tau}
\end{equation}
where $\mathcal{T}$ is a threshold function defined by:
\begin{equation}
\mathcal{T}[a] = \begin{cases}
1 \quad \text{if } |a| < 0.5 \\
0 \quad \text{if } |a| \ge 0.5
\end{cases}
\end{equation}
and $\mathcal{M}$ maps a frequency in Hertz to a melodic 
axis as a real-valued number of semitones above an arbitrary
reference frequency $f_\text{ref}$:
\begin{equation}
\mathcal{M}(f) = 12 \log_2\left( \frac{f}{f_\text{ref}} \right)
\end{equation}

  \item \textbf{Raw Chroma Accuracy}: As raw pitch accuracy, except
  that both the estimated and reference $f_0$ sequences are mapped onto a
  single octave. This gives a measure of pitch accuracy which ignores octave
  errors, a common error made by melody extraction systems:
\begin{equation}
\text{Acc}_\text{chroma}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\left<\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau)\right>_{12} \right] }{\sum_\tau v^*_\tau}
\end{equation}
Octave equivalence is achieved by taking the difference between the semitone-scale pitch 
values modulo 12 (one octave), where
\begin{equation}
\left<a\right>_{12} = a - 12 \lfloor \frac{a}{12} + 0.5 \rfloor .
\end{equation}

  \item \textbf{Overall Accuracy}: this measure combines the performance of the
  pitch estimation and voicing detection tasks to give an overall performance
  score for the system. It is defined as the proportion of all frames
   correctly estimated by the algorithm, where for non-melody
  frames this means the algorithm labeled them as non-melody, and for melody
  frames the algorithm both labeled them as melody frames and provided a correct
  $f_0$ estimate for the melody (i.e.~within half a semitone of the
  reference):

\begin{equation}
\text{Acc}_\text{ov} 
= \frac{1}{L} \sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] + \bar{v}^*_\tau \bar{v}_\tau 
\end{equation}
where $L$ is the total number of frames.
\end{itemize}

The performance of an algorithm on an entire music collection for a given
measure is obtained by averaging the per-excerpt scores for that measure over
all excerpts in the collection. 

\subsubsection{Discussion}

In the measure definitions provided above, it is assumed that both the estimate
and reference sequences are sampled using the same time grid (hop size). In
practice, however, this is not always the case, since the time grid of the
reference depends on the hop size used by the pitch tracker that produced
it, and similarly the time grid of the estimate depends on the specific melody
extraction algorithm that produced it. This means that the sequences must be
resampled onto a common time-grid prior to the computation of the measures. For
the MIREX AME task, any sequence (be it reference or estimate) that is not
sampled using a 10 ms hop size is resampled using $0^{th}$ order interpolation,
i.e.~each frequency value in the target sequence is set to its nearest
neighbour (in time) from the source sequence. This kind of interpolation can
potentially have a detrimental effect on the evaluation, depending on the
difference between the source and target time grids. In particular, it can
result in artificially low scores for sequences with rapidly changing pitch
values, such as opera singing with deep vibrato. 

For this reason, the melody evaluator in mir\_eval uses $1^{st}$ order (linear)
interpolation by default in order to map the reference and estimate sequences
onto a common time-grid. Assuming the original timestamps of both sequences
correspond to the \textit{center} of each analysis frame (as they should),
using $1^{st}$ order rather than $0^{th}$ order interpolation means the results
returned by mir\_eval are lower-bounded by the MIREX results and are, in our
view, more accurate.



% \subsection{Figures, Tables and Captions}
% 
% All artwork must be centered, neat, clean, and legible.
% All lines should be very dark for purposes of reproduction and art work should not be hand-drawn.
% The proceedings are not in color, and therefore all figures must make sense in black-and-white form.
% Figure and table numbers and captions always appear below the figure.
% Leave 1 line space between the figure or table and the caption.
% Each figure or table is numbered consecutively. Captions should be Times 10pt.
% Place tables/figures in text as close to the reference as possible.
% References to tables and figures should be capitalized, for example:
% see \figref{fig:example} and \tabref{tab:example}.
% Figures and tables may extend across both columns to a maximum width of 17.2cm.

% \begin{table}
%  \begin{center}
%  \begin{tabular}{|l|l|}
%   \hline
%   String value & Numeric value \\
%   \hline
%   Hello ISMIR  & 2014 \\
%   \hline
%  \end{tabular}
% \end{center}
%  \caption{Table captions should be placed below the table.}
%  \label{tab:example}
% \end{table}

%\begin{figure}
% \centerline{\framebox{
% \includegraphics[width=\columnwidth]{figure.png}}}
% \caption{Figure captions should be placed below the figure.}
% \label{fig:example}
%\end{figure}



% \begin{thebibliography}{citations}
% 
% \bibitem {Author:00}
% E. Author:
% ``The Title of the Conference Paper,''
% {\it Proceedings of the International Symposium
% on Music Information Retrieval}, pp.~000--111, 2000.
% 
% \bibitem{Someone:10}
% A. Someone, B. Someone, and C. Someone:
% ``The Title of the Journal Paper,''
% {\it Journal of New Music Research},
% Vol.~A, No.~B, pp.~111--222, 2010.
% 
% \bibitem{Someone:04} X. Someone and Y. Someone: {\it Title of the Book},
%     Editorial Acme, Porto, 2012.
% 
% \end{thebibliography}

\bibliography{mir_eval}

\end{document}
