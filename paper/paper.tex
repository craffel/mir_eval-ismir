% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{braket}
%\usepackage{authblk}
\usepackage{booktabs}

\usepackage{url}
\usepackage{microtype}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}
\DeclareMathOperator*{\median}{median}
\newcommand{\ind}[1]{\ensuremath{\left\llbracket#1\right\rrbracket}}
\def\given{\ensuremath{|~}}
\def\defeq{\ensuremath{:=}}

\def\mireval{\texttt{mir\char`_eval}}

% ------
\title{\mireval{}}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%\author[1]{Colin Raffel}
%\author[1]{Brian McFee}
%\author[2]{Justin Salamon}
%\author[2]{Eric J. Humphrey}
%\author[2]{Oriol Nieto}
%\author[1]{Dawen Liang}
%\author[1]{Daniel P. W. Ellis}
%\affil[1]{LabROSA, Dept. of Electrical Engineering\\Columbia University, New York}
%\affil[2]{Music and Audio Research Lab\\New York University, New York}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Central to the field of MIR research is the evaluation of algorithms used to extract information from music data.
We present \mireval{}, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms.
In this paper, we enumerate the metrics implemented by \mireval{} and quantitatively compare each to  existing implementations.
When the scores reported by \mireval{} differ substantially from the reference, we detail the differences in implementation.
We also provide a brief overview of \mireval{}'s architecture, design, and intended use.
\end{abstract}
%
\section{Evaluating MIR Algorithms}

Much of the research in Music Information Retrieval (MIR) involves the development of systems that process raw music data to produce semantic information.
The goal of these systems is frequently defined as attempting to duplicate the performance of a human listener given the same task \cite{downie2003toward}.
A natural way to determine a system's effectiveness might be for a human to study the output produced by the system and judge its correctness.
However, this would yield only subjective ratings, and would also be extremely time-consuming when evaluating a system's output over a large corpus of music.

Instead, objective metrics are developed to provide a well-defined way of computing a score which indicates each system's output's correctness.
These metrics typically involve a heuristically-motivated comparison of the system's output to a reference which is known to be correct.
Over time, certain metrics have become standard for each task, so that the performance of systems created by different researchers can be compared when they are evaluated over the same dataset \cite{downie2003toward}.
Unfortunately, this comparison can be confounded by small details of the implementations or procedures that can have disproportionate impacts on the resulting scores.

% MIREX is the standard, but it's not community-developed, not transparent, not well-documented, uses multiple languages, is coupled to NEMA, not easy to use, dependencies, not used outside of MIREX, etc.
For the past 10 years, the yearly Music Information Retrieval Evaluation eXchange (MIREX) has been a forum for comparing MIR algorithms over common datasets \cite{downie2008music}.
By providing a standardized shared-task setting, MIREX has become critically useful to track progress in MIR research.
MIREX is built upon the Networked Environment for Music Analysis (NEMA) \cite{west2010networked}, a large-scale system which includes exhaustive functionality for evaluating, summarizing, and displaying evaluation results.
The NEMA codebase includes multiple programming languages and dependencies (some of which, e.g. Matlab, are proprietary) so compiling and running it at individual sites is nontrivial. 
%
%Due to its scale, characteristics, and intended use, 
In consequence, the NEMA system is rarely used for evaluating MIR algorithms outside of the setting of MIREX \cite{downie2008music}.
Instead, researchers often create their own implementations of common metrics for evaluating their algorithms.
These implementations are thus not standardized, and may contain differences in details, or even bugs, that confound comparisons.

These factors motivate the development of a standardized software package which implements the most common metrics used to evaluate MIR systems.
Such a package should be straightforward to use and well-documented so that it can be easily adopted by MIR researchers.
In addition, it should be community-developed and transparently implemented so that all design decisions are easily understood and open to discussion and improvement.

Following these criteria, we present \mireval{}, a software package which intends to provide an easy and standardized way to evaluate MIR systems.
This paper first discusses the architecture and design of \mireval{} in Section \ref{sec:architecture}, then, in Section \ref{sec:tasks}, describes all of the tasks covered by \mireval{} and the metrics included.
In order to validate our implementation decisions, we compare \mireval{} to existing software in Section \ref{sec:comparison}.
Finally, we discuss and summarize our contributions in Section \ref{sec:discussion}.

\section{\mireval{}'s architecture}
\label{sec:architecture}

\mireval{} is a Python library which includes metrics for the following tasks: Beat detection, chord recognition, pattern discovery, structural segmentation, melody extraction, and onset detection.
Each task is given its own submodule (e.g. \mireval{}\texttt{.beat}), and each metric is defined as a separate function in each submodule (e.g. \mireval{}\texttt{.beat.f\char`_measure}).
Each task submodule also includes common data pre-processing steps for the task.
Every metric function includes detailed documentation, example usage, input validation, and references to the original paper which defined the metric.
\mireval{} also includes a submodule \texttt{io} which provides convenience functions for loading in task-specific data from common file formats.
For readability, all code follows the PEP8 style guide\cite{van2001pep}.
\mireval{}'s only dependencies outside of the Python standard library are the free and open-source SciPy/Numpy\cite{jones2001scipy} and scikit-learn\cite{pedregosa2011scikit} Python libraries.

In order to simplify the usage of \mireval{}, it is packaged with a set of ``evaluator'' scripts, one for each task.
These scripts include all code necessary to load in data, pre-process it, and compute all metrics for a given task.
The evaluators allow for \mireval{} to be called directly from the command line so that no knowledge of Python is necessary.
They are also distributed as executables for Windows and Mac OS X, so that \mireval{} may be used with no dependencies installed.

\section{Tasks included in \mireval{}}
\label{sec:tasks}

\subsection{Beat Detection}

The aim of a beat detection algorithm is to report the times at which a typical human listener might tap their foot to a piece of music.
As a result, most metrics for evaluating the performance of beat tracking systems involve computing the error between the estimated beat times and some reference list of beat locations.
Many metrics additionally compare the beat sequences at different metric levels in order to deal with the ambiguity of tempo \cite{levy2011improving}.

\mireval{} includes the following metrics for beat tracking, which are defined in detail in \cite{davies2009evaluation}:
The \textbf{F-measure} of the beat sequence, where an estimated beat is considered correct if it is sufficiently close to a reference beat;
\textbf{Cemgil's score}, which computes the sum of Gaussian errors for each beat;
\textbf{Goto's score}, a binary score which is 1 when some specific heuristic criteria are met;
\textbf{McKinney's P-score}, which computes the cross-correlation of the estimated and reference beat sequences represented as impulse trains;
\textbf{continuity-based scores} which compute the proportion of the beat sequence which is continuously correct;
and finally the \textbf{information gain} of the beat error histogram to a uniform distribution.

\subsection{Chord Recognition}

% relevant papers
% - pauwels & peeters
% - matthais
% - harte
% - mirex 
% - Utrecht agreement

Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord recognition is an ongoing topic of discussion.
%For example, ISMIR 2010 saw the fondly dubbed ``Utrecht Agreement on Chord Evaluation''\footnote{http://www.music-ir.org/mirex/wiki/The\_Utrecht\_Agreement\_on\_Chord\_Evaluation}, in which interested researchers met to discuss sane ways of quantifying the performance of automatic chord recognition systems.
Several recent articles address issues and concerns with vocabularies, comparison semantics, and other lexicographical challenges unique to chord recognition \cite{}.
Ultimately, the source of this difficulty stems from the inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation \cite{McVicar}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, e.g Major-Minor, Sevenths, with or without inversions, and so on.

Thanks to the previous efforts of Harte \cite{}, text representations of chord labels adhere to a standardized format, consisting of a root, quality, extensions, and a bass note; of these, only the root is strictly required.
However, in order to efficiently compare chords in a variety of different ways, it is helpful to first translate a given chord label $\mathcal{C}$ into a numerical representation, shown in Figure \ref{}.
In this example, a $G:7(9)/5$ is mapped to split into 4 pieces of information: one, the root is mapped to an absolute pitch class $\mathcal{R}$, in $[0, 11]$, where $C\to0$, $C\sharp/D\flat\to1$, etc; two, the quality is mapped to a root-invariant 12-dimensional bit vector $\mathcal{Q}$ by setting the scale degrees of the quality; three, any extensions are applied (via addition or omission) to the quality bit vector as scale degrees in a single octave, resulting in pitch vector $\mathcal{P}$; and four, the bass interval (5) is translated to the relative scale degree in semitones $\mathcal{B}$.
Note that the add-9 is rolled into a single octave as an add-2.
This is a matter of convenience as extended chords (9's, 11's or 13's) are traditionally resolved to a single-octave equivalent, but the bit-vector representation could be easily expanded to represent such information.

Having gone through this bit of effort, it is now straightforward to compare chords along the five rules used in MIREX 2013:
\begin{enumerate}
\item Root:
	\begin{enumerate}
	\item $\mathcal{R}_{est} == \mathcal{R}_{ref}$
	\item $\forall \mathcal{Q}_{ref}$
	\end{enumerate}
\item Major-Minor: Rule 1.a, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref}$
	\item $\mathcal{Q}_{ref} \in \{Maj, min\}$ 
	\end{enumerate}
\item Major-Minor w/Inversions: Rule 2, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$
	\end{enumerate}
\item Sevenths: Rule 1.1, plus
	\begin{enumerate}
	\item $\mathcal{Q}_{est} == \mathcal{Q}_{ref} $
	\item $ \mathcal{Q}_{ref} \in \{Maj, min, Maj7, min7, 7\}$ 
	\end{enumerate}
\item Sevenths w/Inversions: Rule 4, plus
	\begin{enumerate}
	\item $\mathcal{B}_{ref} \in \mathcal{Q}_{ref}$ 
	\end{enumerate}
\end{enumerate}

Following recent trends in MIREX, an overall score is computed by weighting each comparison by the duration of its interval, over all intervals; stated another way, this is the piecewise continuous-time integral of the intersection of two chord sequences, $(\mathbf{C}_{ref}, \mathbf{C}_{est})$, expressed as follows:

\begin{equation}
S(\mathbf{C}_{ref}, \mathbf{C}_{est}) = \frac{1}{T}\int_{t=0}^{T} \mathcal{C}_{ref}(t) == \mathcal{C}_{est}(t)
\end{equation}

\noindent Here, this is achieved here by forming the union of the boundaries in each sequence, and summing the time intervals of the correct ranges. Note that equivalence is subject to one of the rules defined previously. 

Finally, the total score over a set of $N$ items is given by a discrete summation, where the importance of each score, $S_n$, is weighted by the duration, $T_n$, of each annotation:

\begin{equation}
S_{total} = \frac{\sum_{n=0}^{N} T_n*S_n}{\sum_{n=0}^{N} T_n}
\end{equation}
 
\subsection{Pattern Discovery}

This task aims to evaluate algorithms that identify musical patterns (i.e. short fragments or melodic ideas that repeat at least twice) both from audio and symbolic representations.
Given the novelty of this task \footnote{The first year to appear on MIREX was 2013.}, the evaluation metrics are likely to be modified in further editions.
Nonetheless, Collins put together all the previously existent metrics and a few new ones for this task in its first appearance in MIREX\cite{Collins2013}, which resulted in 19 different scores, each one (except for the two that evaluate the algorithm execution time) implemented in MIR-eval and described below:

\begin{itemize}
    \item
      \textbf{Standard F-measure, Precision, and Recall (F$_1$, P, R)}: This metric, composed of three scores, checks if the prototype patterns of the reference match possible key-transposed patterns in the prototype patterns of the estimations.
      Since the sizes of these prototypes must be equal, this metric is quite restrictive and it tends to be 0 most of the time (see 2013 MIREX results).
    \item
      \textbf{Establishment F-measure, Precision, and Recall (F1$_{est}$, P$_{est}$, R$_{est}$)}: These scores evaluate the amount of patterns that were successfully identified by the estimated results, no matter how many occurrences they found.
      In other words, this metric captures the ability of the algorithm to \textit{establish} that the estimated patterns are actually contained in the reference annotation.
    \item 
      \textbf{Occurrence F-measure, Precision, and Recall (F1$_{occ}$, P$_{occ}$, R$_{occ}$)}: Independently of how many patterns were correctly established, we may also want to know how well the algorithm finds all their respective occurrences throughout the piece.
      This metric aims to quantize this \textit{occurrence} retrieval quality of the algorithm.
      This metric has a tolerance parameter $c$ to allow differences when evaluating the similarity between occurrences, and in MIREX they use $c=.75$ and $c=.5$.
    \item
      \textbf{Three-layer F-measure, Precision, and Recall (TLF$_1$, P$_3$, R$_3$)}: These scores can be seen as an improved version of the standard metrics.
      They capture both the establishment of the patterns and the occurrence retrieval in a single set of scores, being more permissive than the standard evaluation.
    \item
      \textbf{First $N$ patterns metrics (FFTP$_{est}$, FFP)}: This includes the first $N$ patterns target proportion establishment recall, and the first $N$ patterns three-layer precision. 
      By analyzing the first $N$ patterns only, we evaluate the ability of the algorithm of sorting the identified patterns based on their relevance. 
      In MIREX, $N=5$.

\end{itemize}

\subsection{Segmentation}

Evaluation criteria for segmentation fall into two categories: boundary annotation, and structural annotation.
\emph{Boundary annotation} is the task of predicting the times at which structural changes occur, such as when a \emph{verse} transitions to a \emph{refrain}.
\emph{Structural annotation} is the task of assigning labels to detected segments.  
The estimated labels may be arbitrary strings --- such as $A$, $B$, $C$, \etc{} --- and they need not describe functional concepts.

In both tasks, we assume that annotations express a partitioning of the track 
into intervals ${(s_i, t_i)}_{i=1}^m$, where $s_0=0$ denotes the beginning of track, 
$t_m$ denotes the end of the track, and $t_i = s_{i+1}$.
Structural annotation additionally requires that each interval be assigned a label $y_i$.

\mireval{} implements the following MIREX-compatible boundary detection metrics:
\begin{description}
\item[Boundary detection:] precision, recall, and F-measure for detecting boundary
events within a tolerance window $w\in \{0.5, 3\}$~\cite{turnbull2007supervised};
\item[Boundary deviation:] median absolute time difference from a reference boundary
to its nearest estimated boundary, and vice versa~\cite{turnbull2007supervised},
\end{description}
and the following structure annotation metrics
\begin{description}
\item[Pairwise classification:] precision, recall, and F-measure for classifying pairs
of sampled time instants as belonging to the same structural
component~\cite{levy2008structural};
\item[Rand index:\footnote{The MIREX results page refers to Rand index as ``random
clustering index''.}] reference and estimated annotations are sampled uniformly
throughout the track, and the induced clusterings are compared by the Rand index
~\cite{rand1971objective};
\item[Normalized conditional entropy:] reference and estimated labels are sampled
uniformly, and interpreted as samples of random variables $Y_R, Y_E$, which are
computed by estimating the conditional entropy of $Y_R$ given $Y_E$
(\emph{under-segmentation}) and $Y_E$ given $Y_R$ 
(\emph{over-segmentation})~\cite{lukashevich2008towards}.
\end{description}

% \subsubsection{Boundary annotation}

% Within boundary annotation, there are two categories of evaluation metrics: detection,
% and deviation~\cite{turnbull2007supervised}.  

% \emph{Boundary detection} measures the precision, recall, and F-measure of boundary prediction within a window.
% Let $B^R$ ($B^E$) denote the set of unique interval boundaries in the reference (estimated) annotation, and let $W$ denote 
% a window, typically either $0.5$s or $3.0$s.  
% A \emph{hit} is defined as a pair $b_e \in B^E$, $b_r \in B^R$ such that $|b_e - b_r|
% \leq W$.  

% No estimated boundary is counted as a hit for more than one reference boundary, and vice versa.
% This is accomplished by computing a maximum bipartite matching $H_W$ between $B^E$ and $B^R$ (subject to the window constraint $W$)
% using the Hopcroft-Karp algorithm~\cite{hopcroft1973n}.  This deviates from the greedy heuristic used in the MIREX
% implementation, but it is guaranteed to produce a correct matching.  Precision and recall at window $W$ are defined as
% \begin{align}
% P_W(B^R, B^E) &\defeq \frac{|H_W|}{|B^E|}\\
% R_W(B^R, B^E) &\defeq \frac{|H_W|}{|B^R|},
% \end{align}
% and the corresponding F-measure is defined by taking their harmonic mean.

% \emph{Boundary deviation} is comprised of two scores, which measure the median time between a reference boundary and its nearest corresponding estimated boundary, and vice versa.
% These two metrics have previously been defined as \emph{true-to-predicted} (T-to-P) and \emph{predicted-to-true} (P-to-T), though in the terminology of this document, we use 
% alternative naming of \emph{reference-to-estimated} (R-to-E) and \emph{estimated-to-reference} (E-to-R):
% \begin{align}
% \text{R-to-E}(B^R, B^E) &\defeq \displaystyle\median_{b_r \in B^R} \min_{b_e \in B^E} \left|b_r - b_e\right|\\
% \text{E-to-R}(B^R, B^E) &\defeq \displaystyle\median_{b_e \in B^E} \min_{b_r \in B^R} \left|b_r - b_e\right|.
% \end{align}


% \subsubsection{Structural annotation}
% Two standard methods of evaluating structural annotation accuracy are \emph{pairwise classification}~\cite{levy2008structural}
% and conditional entropy~\cite{lukashevich2008towards}.  In both methods, a collection of samples are generated by sampling the
% labels at time steps between $0$ and the track duration.  The $i$th sample is assigned a reference label $y_i^R$ and
% and estimated label $y_i^E$.  Our implementation follows the MIREX guidelines, and generates samples at a default rate of 10Hz.

% Reference and estimated annotations must span identical time durations for annotation metrics to be well-defined.
% This is accomplished in our implementation by trimming or padding the estimated annotation to exactly match the start and end-times
% reported in the reference annotation, and synthesizing unique labels if necessary.

% Given the labels for the samples $\{(y_i^R, y_i^E)\}_{i=1}^n$, the \emph{pairwise classification} metrics are defined as precision,
% recall, and F-measure of label agreement over all unique, distinct pairs $i \neq j$.  
% Let $A_R = \{ \{i, j\} \given y_i^R = y_j^R\}$ denote the set of similarly labeled frames in the reference, with $A_E$ defined
% analogously for the estimation.  Precision and recall are defined as
% \begin{align}
% P_\text{pair} &\defeq \frac{ |A_E \cap A_R| }{ |A_E| }\\
% R_\text{pair} &\defeq \frac{ |A_E \cap A_R| }{ |A_R| }.
% \end{align}

% \emph{Normalized conditional entropy} scores are computed by estimating the conditional entropy of the estimated label $y^E$ given
% the reference label $y^R$ and vice versa.  Let $P$ denote the empirical joint distribution over reference and estimated labels:
% \begin{align}
% P_{ij} &\propto \left|\{t \given y_t^R = i \wedge y_t^E = j\}\right|.
% \end{align}
% Let $Y^R$ and $Y^E$ denote random variables corresponding to the reference and estimated label for an arbitrary sample.
% The conditional entropies $H\left(Y^E \given Y^R\right)$ and $H\left(Y^R \given Y^E\right)$ are estimated from the 
% empirical marginals ${p_i^R = \sum_j P_{ij}}$ and ${p_j^E = \sum_i P_{ij}}$ as described by 
% Lukashevich~\cite{lukashevich2008towards}.

% The final scores are defined as \emph{over-segmentation} ($S_\text{O}$) and \emph{under-segmentation} ($S_\text{U}$):
% \begin{align}
% S_\text{O} &\defeq 1 - \frac{H(E \given R)}{\log_2 |\ell^E|}\\
% S_\text{U} &\defeq 1 - \frac{H(R \given E)}{\log_2 |\ell^R|},
% \end{align}
% where $\ell^R$ and $\ell^E$ denote the sets of unique label values given in the reference and estimation.

\subsection{Melody Extraction}
\subsubsection{Task definition}
Melody extraction algorithms aim to produce a sequence of frequency values
corresponding to the pitch of the dominant melody from a musical recording
\cite{salamon:MelodyReview:IEEESPM13}. The estimated pitch is represented as a
time series of fundamental frequency ($f_0$) values in Hz sampled on a fixed
time grid (e.g.~every 10 ms). To evaluate the estimated sequence, a reference
sequence is generated by running a pitch tracker on the monophonic melody track
(requiring access to the multi-track recording session) and manually correcting
any mistakes made by the pitch tracker. The estimate is then evaluated against
the reference by comparing the two frequency sequences on a frame-by-frame
basis, and computing five global measures, first used in the MIREX 2005 AME
evaluation \cite{polinerMelodyEval}. The goal of these measures, defined below,
is to assess the algorithm's performance on two subtasks of melody extraction:
(1) pitch estimation, i.e.~how well the algorithm estimates the pitch of the
melody, and (2) voicing detection, i.e.~how well the algorithm determines when
the melody is present in a frame (a \textit{voiced} frame) or absent (an
\textit{unvoiced} frame). To allow evaluation of these two subtasks
independently, a melody extraction algorithm can provide a frequency estimate
even for frames it has determined to be unvoiced.

\subsubsection{Evaluation measures}
The following definitions are taken from
\cite{salamon:MelodyReview:IEEESPM13} with permission from the authors. Let the estimated melody pitch frequency
vector be $\mathbf{f}$, and the true sequence (reference) be $\mathbf{f}^*$. Let
us also define a voicing indicator vector $\mathbf{v}$, whose $\tau^\text{th}$
element $v_\tau = 1$ when a melody pitch is detected, with corresponding ground
truth $\mathbf{v}^*$.  We also define an ``unvoicing'' indicator $\bar{v}_\tau =
1 - v_\tau$.  Recall that an algorithm may report an estimated melody pitch
($f_\tau > 0$) even for times where it reports no voicing ($v_\tau = 0$).   
Then the measures are:
\begin{itemize}
  \item \textbf{Voicing Recall Rate}: The proportion of frames labeled as
  melody frames in the reference that are estimated as melody frames by the
  algorithm.
\begin{equation}
\text{Rec}_\text{vx} = \frac{\sum_\tau v_\tau v^*_\tau}{\sum_\tau v^*_\tau}
\end{equation}

  \item \textbf{Voicing False Alarm Rate}: The proportion of frames
  labeled as non-melody in the reference that are mistakenly estimated as
  melody frames by the algorithm.
\begin{equation}
\text{FA}_\text{vx} = \frac{\sum_\tau v_\tau \bar{v}^*_\tau}{\sum_\tau \bar{v}^*_\tau}
\end{equation}

  \item \textbf{Raw Pitch Accuracy}: The proportion of melody frames in the
  reference for which $f_\tau$ is considered
  correct (i.e.~within half a semitone of the ground
  truth $f^*_\tau$).
\begin{equation}
\text{Acc}_\text{pitch}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] }{\sum_\tau v^*_\tau}
\end{equation}
where $\mathcal{T}$ is a threshold function defined by:
\begin{equation}
\mathcal{T}[a] = \begin{cases}
1 \quad \text{if } |a| < 0.5 \\
0 \quad \text{if } |a| \ge 0.5
\end{cases}
\end{equation}
and $\mathcal{M}$ maps a frequency in Hertz to a melodic 
axis as a real-valued number of semitones above an arbitrary
reference frequency $f_\text{ref}$:
\begin{equation}
\mathcal{M}(f) = 12 \log_2\left( \frac{f}{f_\text{ref}} \right)
\end{equation}

  \item \textbf{Raw Chroma Accuracy}: As raw pitch accuracy, except
  that both the estimated and reference $f_0$ sequences are mapped onto a
  single octave. This gives a measure of pitch accuracy which ignores octave
  errors, a common error made by melody extraction systems:
\begin{equation}
\text{Acc}_\text{chroma}= \frac{\sum_\tau v^*_\tau \mathcal{T}\left[\left<\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau)\right>_{12} \right] }{\sum_\tau v^*_\tau}
\end{equation}
Octave equivalence is achieved by taking the difference between the semitone-scale pitch 
values modulo 12 (one octave), where
\begin{equation}
\left<a\right>_{12} = a - 12 \lfloor \frac{a}{12} + 0.5 \rfloor .
\end{equation}

  \item \textbf{Overall Accuracy}: this measure combines the performance of the
  pitch estimation and voicing detection tasks to give an overall performance
  score for the system. It is defined as the proportion of all frames
   correctly estimated by the algorithm, where for non-melody
  frames this means the algorithm labeled them as non-melody, and for melody
  frames the algorithm both labeled them as melody frames and provided a correct
  $f_0$ estimate for the melody (i.e.~within half a semitone of the
  reference):

\begin{equation}
\text{Acc}_\text{ov} 
= \frac{1}{L} \sum_\tau v^*_\tau \mathcal{T}\left[\mathcal{M}(f_\tau)-\mathcal{M}(f^*_\tau) \right] + \bar{v}^*_\tau \bar{v}_\tau 
\end{equation}
where $L$ is the total number of frames.
\end{itemize}

The performance of an algorithm on an entire music collection for a given
measure is obtained by averaging the per-song scores for that measure over
all songs in the collection. 

\subsubsection{Discussion}

In the measure definitions provided above, it is assumed that both the estimate
and reference sequences are sampled using the same time grid (hop size). In
practice, however, this is not always the case, since the time grid of the
reference depends on the hop size used by the pitch tracker that produced
it, and similarly the time grid of the estimate depends on the specific melody
extraction algorithm that produced it. This means that the sequences must be
resampled onto a common time-grid prior to the computation of the measures. For
the MIREX AME task, any sequence (be it reference or estimate) that is not
sampled using a 10 ms hop size is resampled using $0^{th}$-order interpolation,
i.e.~each frequency value in the target sequence is set to its nearest
neighbour (in time) from the source sequence. This kind of interpolation can
potentially have a detrimental effect on the evaluation, depending on the
difference between the source and target time grids. In particular, it can
result in artificially low scores for sequences with rapidly changing pitch
values, such as opera singing with deep vibrato. 

For this reason, the melody evaluator in \mireval{} uses $1^{st}$-order (linear)
interpolation by default in order to map the reference and estimate sequences
onto a common time-grid. Assuming the original timestamps of both sequences
correspond to the \textit{center} of each analysis frame (as they should),
using $1^{st}$-order rather than $0^{th}$-order interpolation means the results
returned by \mireval{} are lower-bounded by the MIREX results and are, in our
view, more accurate. By default, the estimate is resampled using the reference's
time grid so that the reference can be used without any modification.
Alternatively, the user can specify a hop size, in which case both
reference and estimate are resampled onto the new time-grid defined by the hop
size.

\subsection{Onset Detection}

The goal of an onset detection algorithm is to automatically determine when notes are played in a piece of music.
As is also done in beat tracking and segment boundary detection, the primary method used to evaluate onset detectors is to first determine which estimated onsets are ``correct'', where correctness is defined as being within a small window of a reference onset \cite{bock2012evaluating}.
From this, \textbf{precision}, \textbf{recall}, and \textbf{F-measure} scores are computed.

\section{Comparison to Existing Implementations}
\label{sec:comparison}

In order to validate the design choices made in \mireval{}, it is useful to compare the scores it reports to those reported by an existing evaluation system.
Beyond pinpointing intentional differences in implementation, this process can also help find and fix bugs in either \mireval{} or the system it is being compared to.

For each task covered by \mireval{}, we obtained a collection of reference and estimated annotations and computed a score for each metric using \mireval{} and the evaluation system being compared to.
In order to facilitate comparison, we ensured that all parameters and pre-processing used by \mireval{} were equivalent to the reference system unless otherwise explicitly noted.
Then, for each reported score, we computed the relative change between the scores as their absolute difference divided by their mean, or
$$
\frac{|s_m - s_c|}{(s_m + s_c)/2}
$$
where $s_m$ is the score reported by \mireval{} and $s_c$ is the score being compared to.
We then computed the average relative change across all examples in the obtained dataset for each score.

For the beat detection, chord recognition, structural segmentation, and onset detection tasks, MIREX releases the the output of submitted algorithms, the ground truth annotations, and the reported score for each example in each data set.
We therefore can directly compare \mireval{} to MIREX for these tasks by collecting all reference and estimated annotations, computing each metric for each example, and comparing the result to the score reported by MIREX.
We chose to compare against the results reported in MIREX 2013 for all tasks.

In contrast to the tasks above, MIREX does not release ground truth annotations or algorithm output for the melody extraction and pattern discovery tasks.
As a result, we compared \mireval{}'s output on smaller development datasets for these tasks.
For melody extraction, the ADC2004 dataset used by MIREX is publicly available.
We performed melody extraction using the ``SG2'' algorithm evaluated in 2011 \cite{salamon2011melody} and compared \mireval{}'s reported scores to those of MIREX.
For pattern discovery, we used the development dataset released by Collins \cite{Collins2013} and used the algorithms submitted by Nieto and Farbood \cite{nieto2013discovery} for MIREX 2013 to produce estimated patterns.
We evaluated the estimated patterns using the MATLAB code released by Collins \cite{Collins2013}.
The number of algorithms, examples, and total number of scores for all tasks are summarized in Table \ref{tab:nexamples}.

The resulting average relative change for each metric is presented in Table \ref{tab:comparison}.
The average relative change for all of the pattern discovery metrics was 0, so they are not included in this table.
For many of the other metrics, the average relative change was less than a few tenths of a percent, indicating that \mireval{} is equivalent up to rounding/precision errors.
In the following sections, we enumerate the known implementation differences which account for the larger average relative changes.

\subsection{Non-greedy matching of events}

Brian writes this section, which covers the differences in beat.f\_measure, boundary.detection, and onset.f\_measure

\subsection{McKinney's P-score}

When computing McKinney's P-score \cite{davies2009evaluation}, the beat sequences are first converted to impulse trains sampled at a 10 millisecond resolution.
Because this sampling involves quantizing the beat times to a 10ms grid, shifting both beat sequences by a constant offset can result in substantial changes in the P-score.
As a result, In \mireval{}, we normalize the beat sequences by subtracting from each reference and estimated beat location the minimum beat location in either series.
In this way, the smallest beat in the estimated and reference beat sequences is always $0$ and the metric remains the same even when both beat sequences have a constant offset applied.
This is not done in MIREX (which uses the Beat Evaluation Toolbox \cite{davies2009evaluation}), and as a result, we observe a considerable average relative change for the P-score metric.

\subsection{Information Gain}

The Information Gain metric \cite{davies2009evaluation} involves the computation a histogram of the per-beat errors.
The Beat Evaluation Toolbox uses a non-uniform histogram binning where the first, second and last bins are smaller than the rest of the bins while \mireval{} uses a standard uniformly-binned histogram.
As a result, the Information Gain score reported by \mireval{} differs substantially from that reported by MIREX.

\subsection{Other segmentation differences}

Brian renames this section and writes it/splits it into different sections.

\subsection{Chord stuff}

Eric renames this section and writes it/splits it into different sections

\subsection{Voicing False Alarm Rate}

When a reference melody annotation contains no unvoiced frames, the voicing false alarm rate is not well defined.
MIREX assigns a score of 1 in this case, while \mireval{} assigns a score of 0.
%%%
%%%
%%%
% WHY?
%%%
%%%
%%%
In the data set over which the average relative change for the melody metrics was computed, one reference annotation contained no unvoiced frames.
This discrepancy caused a large inflation of the average relative change reported for the voicing false alarm rate.

\begin{table*}[t]
\label{tab:comparison}
  \centering
\begin{tabular}{c c c c c c c c c}
\toprule
 \multicolumn{9}{ c }{Beat Detection} \\
  \cmidrule(lr){1-9}
F-Measure  & Cemgil     & Goto       &  P-score   & CMLc       & CMLt       & AMLc       & AMLt       & In. Gain   \\
   0.703\% &    0.035\% &    0.054\% &    0.877\% &    0.161\% &    0.143\% &    0.137\% &    0.139\% &    9.174\% \\
  \midrule
\end{tabular}
\begin{tabular}{c c c c c c c c c}
 \multicolumn{9}{ c }{Structural Segmentation} \\
  \cmidrule(lr){1-9}
  NCE-Over   & NCE-under  & Pairwise F & Pairwise P & Pairwise R & Rand       & F@.5       & P@.5       & R@.5  \\
3.182\% &   11.082\% &    0.937\% &    0.942\% &    0.785\% &    0.291\% &    0.429\% &    0.088\% &    1.021\%  \\
  \midrule
\end{tabular}
\begin{tabular}{c c c c c c c c}
 \multicolumn{5}{ c }{Structural Segmentation (continued)} & \multicolumn{3}{ c }{Onset Detection}\\
  \cmidrule(lr){1-5}
  \cmidrule(lr){6-8}
  F@3 &   P@3        & R@3 & Ref-est dev. & Est-ref dev. & F-measure  & Precision  & Recall     \\
 0.393\%  & 0.094\% &    0.954\% & 0.935\% &    0.000\% &  0.165\% &    0.165\% &    0.165\% \\
  \midrule
\end{tabular}
\begin{tabular}{c c c c c c c c c c}
 \multicolumn{5}{ c }{Chord Recognition} & \multicolumn{5}{ c }{Melody Extraction}\\
  \cmidrule(lr){1-5}
  \cmidrule(lr){6-10}
  Root       & Maj/min       & Maj/min + Inv & 7ths & 7ths + Inv & Overall & Raw Pitch & Chroma & Voicing R & Voicing FA \\
  ??? & ??? & ??? & ??? & ??? & 0.070\% &    0.087\% &    0.114\% &    0.000\% &   10.095\% \\
  \bottomrule
\end{tabular}
\end{table*}

\section{Discussion}
\label{sec:discussion}

% Community development is crucial

\bibliography{mir_eval}

\end{document}
