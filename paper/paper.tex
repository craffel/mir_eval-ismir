% Switch chord footnote to ref


% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{stmaryrd}
\usepackage{braket}
%\usepackage{authblk}
\usepackage{booktabs}

\usepackage{url}
\usepackage{microtype}
\usepackage{tabularx}

\def\eg{\emph{e.g.}}
\def\ie{\emph{i.e.}}
\def\etc{\emph{etc.}}
\def\etal{\emph{et al.}}
\DeclareMathOperator*{\median}{median}
\newcommand{\ind}[1]{\ensuremath{\left\llbracket#1\right\rrbracket}}
\def\given{\ensuremath{|~}}
\def\defeq{\ensuremath{:=}}

\def\mireval{\texttt{mir\char`_eval}}

% ------
\title{\mireval{}}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

%\author[1]{Colin Raffel}
%\author[1]{Brian McFee}
%\author[2]{Justin Salamon}
%\author[2]{Eric J. Humphrey}
%\author[2]{Oriol Nieto}
%\author[1]{Dawen Liang}
%\author[1]{Daniel P. W. Ellis}
%\affil[1]{LabROSA, Dept. of Electrical Engineering\\Columbia University, New York}
%\affil[2]{Music and Audio Research Lab\\New York University, New York}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Central to the field of MIR research is the evaluation of algorithms used to extract information from music data.
We present \mireval{}, an open source software library which provides a transparent and easy-to-use implementation of the most common metrics used to measure the performance of MIR algorithms.
In this paper, we enumerate the metrics implemented by \mireval{} and quantitatively compare each to  existing implementations.
When the scores reported by \mireval{} differ substantially from the reference, we detail the differences in implementation.
We also provide a brief overview of \mireval{}'s architecture, design, and intended use.
\end{abstract}
%
\section{Evaluating MIR Algorithms}

Much of the research in Music Information Retrieval (MIR) involves the development of systems that process raw music data to produce semantic information.
The goal of these systems is frequently defined as attempting to duplicate the performance of a human listener given the same task \cite{downie2003toward}.
A natural way to determine a system's effectiveness might be for a human to study the output produced by the system and judge its correctness.
However, this would yield only subjective ratings, and would also be extremely time-consuming when evaluating a system's output over a large corpus of music.

Instead, objective metrics are developed to provide a well-defined way of computing a score which indicates each system's output's correctness.
These metrics typically involve a heuristically-motivated comparison of the system's output to a reference which is known to be correct.
Over time, certain metrics have become standard for each task, so that the performance of systems created by different researchers can be compared when they are evaluated over the same dataset \cite{downie2003toward}.
Unfortunately, this comparison can be confounded by small details of the implementations or procedures that can have disproportionate impacts on the resulting scores.

% MIREX is the standard, but it's not community-developed, not transparent, not well-documented, uses multiple languages, is coupled to NEMA, not easy to use, dependencies, not used outside of MIREX, etc.
For the past 10 years, the yearly Music Information Retrieval Evaluation eXchange (MIREX) has been a forum for comparing MIR algorithms over common datasets \cite{downie2008music}.
By providing a standardized shared-task setting, MIREX has become critically useful to track progress in MIR research.
MIREX is built upon the Networked Environment for Music Analysis (NEMA) \cite{west2010networked}, a large-scale system which includes exhaustive functionality for evaluating, summarizing, and displaying evaluation results.
The NEMA codebase includes multiple programming languages and dependencies (some of which, e.g. Matlab, are proprietary) so compiling and running it at individual sites is nontrivial. 
%
%Due to its scale, characteristics, and intended use, 
In consequence, the NEMA system is rarely used for evaluating MIR algorithms outside of the setting of MIREX \cite{downie2008music}.
Instead, researchers often create their own implementations of common metrics for evaluating their algorithms.
These implementations are thus not standardized, and may contain differences in details, or even bugs, that confound comparisons.

These factors motivate the development of a standardized software package which implements the most common metrics used to evaluate MIR systems.
Such a package should be straightforward to use and well-documented so that it can be easily adopted by MIR researchers.
In addition, it should be community-developed and transparently implemented so that all design decisions are easily understood and open to discussion and improvement.

Following these criteria, we present \mireval{}, a software package which intends to provide an easy and standardized way to evaluate MIR systems.
This paper first discusses the architecture and design of \mireval{} in Section \ref{sec:architecture}, then, in Section \ref{sec:tasks}, describes all of the tasks covered by \mireval{} and the metrics included.
In order to validate our implementation decisions, we compare \mireval{} to existing software in Section \ref{sec:comparison}.
Finally, we discuss and summarize our contributions in Section \ref{sec:discussion}.

\section{\mireval{}'s architecture}
\label{sec:architecture}

\mireval{} is a Python library which includes metrics for the following tasks: Beat detection, chord estimation, pattern discovery, structural segmentation, melody extraction, and onset detection.
Each task is given its own submodule, and each metric is defined as a separate function in each submodule.
Each task submodule also includes common data pre-processing steps for the task.
Every metric function includes detailed documentation, example usage, input validation, and references to the original paper which defined the metric.
\mireval{} also includes a submodule \texttt{io} which provides convenience functions for loading in task-specific data from common file formats.
For readability, all code follows the PEP8 style guide\cite{van2001pep}.
\mireval{}'s only dependencies outside of the Python standard library are the free and open-source SciPy/Numpy\cite{jones2001scipy} and scikit-learn\cite{pedregosa2011scikit} Python libraries.

In order to simplify the usage of \mireval{}, it is packaged with a set of ``evaluator'' scripts, one for each task.
These scripts include all code necessary to load in data, pre-process it, and compute all metrics for a given task.
The evaluators allow for \mireval{} to be called directly from the command line so that no knowledge of Python is necessary.
They are also distributed as executables for Windows and Mac OS X, so that \mireval{} may be used with no dependencies installed.

\section{Tasks included in \mireval{}}
\label{sec:tasks}

\subsection{Beat Detection}

The aim of a beat detection algorithm is to report the times at which a typical human listener might tap their foot to a piece of music.
As a result, most metrics for evaluating the performance of beat tracking systems involve computing the error between the estimated beat times and some reference list of beat locations.
Many metrics additionally compare the beat sequences at different metric levels in order to deal with the ambiguity of tempo \cite{levy2011improving}.

\mireval{} includes the following metrics for beat tracking, which are defined in detail in \cite{davies2009evaluation}:
The \textbf{F-measure} of the beat sequence, where an estimated beat is considered correct if it is sufficiently close to a reference beat;
\textbf{Cemgil's score}, which computes the sum of Gaussian errors for each beat;
\textbf{Goto's score}, a binary score which is 1 when at least 25\% of the estimated beat sequence closely matches the reference beat sequence;
\textbf{McKinney's P-score}, which computes the cross-correlation of the estimated and reference beat sequences represented as impulse trains;
\textbf{continuity-based scores} which compute the proportion of the beat sequence which is continuously correct;
and finally the \textbf{information gain} of the normalized beat error histogram to a uniform distribution.

\subsection{Chord Estimation}

% relevant papers
% - pauwels & peeters
% - matthais
% - harte
% - mirex 
% - Utrecht agreement

Despite being one of the oldest MIREX tasks, evaluation methodology and metrics for automatic chord estimation (ACE) is an ongoing topic of discussion, due to issues with vocabularies, comparison semantics, and other lexicographical challenges unique to the task \cite{Peeters2013}.
%For example, ISMIR 2010 saw the fondly dubbed ``Utrecht Agreement on Chord Evaluation''\footnote{http://www.music-ir.org/mirex/wiki/The\_Utrecht\_Agreement\_on\_Chord\_Evaluation}, in which interested researchers met to discuss sane ways of quantifying the performance of automatic chord estimation systems.
One source of difficulty stems from an inherent subjectivity in ``spelling'' a chord name and the level of detail a human observer can provide in a reference annotation \cite{McVicar}.
As a result, a consensus has yet to be reached regarding the single best approach to comparing two sequences of chord labels, and instead are often compared over a set of rules, i.e Root, Major-Minor, and Sevenths, with or without inversions.

To efficiently compare chords, we first translate a given chord label $\mathcal{C}$ into a numerical representation, based on the syntax of \cite{Harte2010}.
For example, $G:maj(6)/5$ is mapped to three pieces of information: one, the root (``G'') is mapped to an absolute pitch class $\mathcal{R}$, in $[0, 11]$, where $C\to0$, $C\sharp/D\flat\to1$, etc; two, the quality shorthand (``maj'') and scale degrees (``6'') are mapped to a root-invariant bit vector $\mathcal{S}$ of active semitones; and three, the bass interval (``5'') is translated to the relative scale degree in semitones $\mathcal{B}$.
%Traditionally, extended chords (9's, 11's or 13's) are not considered in evaluation and instead resolved to the tetrad contained therein, e.g. $C:9 \to C:7$, etc. 
%This semitone bit-vector representation can, however, be easily expanded to represent such information in the future.

Based on this representation, we can symbolically define our interpretation of the five rules used in MIREX 2013\footnote{http://www.music-ir.org/mirex/wiki/2013:Audio\_Chord\_Estimation} to compare an estimated chord $\mathcal{C}_{est}$ with a reference $\mathcal{C}_{ref}$.
\textbf{Root} is given by $\mathcal{R}_{est} == \mathcal{R}_{ref}$, for all chords.
\textbf{Major-Minor} includes \textbf{Root}, plus $\mathcal{S}_{est} == \mathcal{S}_{ref}$ subject to $\mathcal{S}_{ref} \in \{Maj, min\}$.
\textbf{Major-Minor-Inv} includes \textbf{Major-minor}, plus $\mathcal{B}_{est} == \mathcal{B}_{ref}$ subject to $\mathcal{B}_{ref} \in \mathcal{S}_{ref}$.
\textbf{Sevenths} follows \textbf{Major-minor}, but is instead subject to $\mathcal{S}_{ref} \in \{Maj, min, Maj7, min7, 7, minmaj7\}$.
Finally, \textbf{Sevenths-Inv} includes \textbf{Sevenths}, plus $\mathcal{B}_{est} == \mathcal{B}_{ref}$ subject to $\mathcal{B}_{ref} \in \mathcal{S}_{ref}$.
Before proceeding, we draw attention to the set-inclusion conditions above, e.g. ``subject to...'', meaning that a comparison is \emph{ignored} during evaluation if the given criteria is not satisfied.

Track-wise scores, $S_n$, are computed by weighting each comparison by the duration of its interval, over all intervals in an audio file; stated another way, this is the piecewise continuous-time integral of the equivalence of two chord sequences, $(\mathbf{C}_{ref}, \mathbf{C}_{est})$, expressed as follows:

\begin{equation}
S_n(\mathbf{C}_{ref}, \mathbf{C}_{est}) = \frac{1}{T}\int_{t=0}^{T} \mathcal{C}_{ref}(t) == \mathcal{C}_{est}(t)
\end{equation}

\noindent This is achieved by forming the union of the boundaries in each sequence, and summing the time intervals of the ``correct'' ranges. 
The cumulative score, referred to as \emph{weighted chord symbol recall} (WCSR), is tallied over a set of $N$ items, i.e. audio files, by discrete summation, where the importance of each score, $S_n$, is weighted by the duration, $T_n$, of each annotation:

\begin{equation}
WCSR_{total} = \frac{\sum_{n=0}^{N} T_n*S_n}{\sum_{n=0}^{N} T_n}
\end{equation}
 
\subsection{Pattern Discovery}

Pattern discovery involves the identification of musical patterns (i.e. short fragments or melodic ideas that repeat at least twice) both from audio and symbolic representations.
The metrics used to evaluation pattern discovery systems attempt to quantify the ability of the algorithm to not only determine the presents pattern in a piece, but also to find all of their occurrences.
Given the novelty of this task (the first year it appeared in MIREX was 2013), the evaluation metrics are likely to be modified in further editions.
Nonetheless, Collins compiled all previously existent metrics and proposed novel ones \cite{Collins2013}, which resulted in 19 different scores, each one implemented in \mireval{}:
\textbf{Standard F-measure, Precision, and Recall}, where an estimated prototype pattern is considered correct only if it matches (up to translation) a reference prototype pattern;
\textbf{Establishment F-measure, Precision, and Recall}, which computes the number of reference patterns that were successfully found, no matter how many occurrences were found;
\textbf{Occurrence F-measure, Precision, and Recall}, which measures whether an algorithm is able to retrieve all occurrences of a pattern;
\textbf{Three-layer F-measure, Precision, and Recall}, which capture both the establishment of the patterns and the occurrence retrieval in a single set of scores;
and the \textbf{First $N$ patterns metrics}, where the target proportion establishment recall and three-layer precision analyzing the first $N$ patterns only, which measures the ability of the algorithm to sort the identified patterns based on their relevance. 

\subsection{Structural Segmentation}

Evaluation criteria for structural segmentation fall into two categories: boundary annotation and structural annotation.
Boundary annotation is the task of predicting the times at which structural changes occur, such as when a verse transitions to a refrain.
Structural annotation is the task of assigning labels to detected segments.
The estimated labels may be arbitrary strings --- such as $A$, $B$, $C$, \etc{} --- and they need not describe functional concepts.
In both tasks, we assume that annotations express a partitioning of the track into intervals.

\mireval{} implements the following boundary detection metrics:
\textbf{Boundary detection precision, recall, and F-measure scores} where an estimated boundary is considered correct if it falls within a window around a reference boundary \cite{turnbull2007supervised};
\textbf{Boundary deviation} which computes median absolute time difference from a reference boundary to its nearest estimated boundary, and vice versa \cite{turnbull2007supervised}
The following structure annotation metrics are also included:
\textbf{Pairwise classification precision, recall, and F-measure scores} for classifying pairs of sampled time instants as belonging to the same structural component~\cite{levy2008structural};
\textbf{Rand index}\footnote{The MIREX results page refers to Rand index as ``random clustering index''.} which clusters reference and estimated annotations and compares themby the Rand index ~\cite{rand1971objective};
and the \textbf{Normalized conditional entropy} where sampled reference and estimated labels are interpreted as samples of random variables $Y_R, Y_E$ from which the conditional entropy of $Y_R$ given $Y_E$ (\emph{under-segmentation}) and $Y_E$ given $Y_R$  (\emph{over-segmentation}) are estimated \cite{lukashevich2008towards}.

\subsection{Melody Extraction}

Melody extraction algorithms aim to produce a sequence of frequency values corresponding to the pitch of the dominant melody from a musical recording \cite{salamon:MelodyReview:IEEESPM13}.
Melody annotations are represented as time series of fundamental frequency ($f_0$) values in Hz sampled at discrete points (``frames'') in time.
An estimated pitch series is evaluated against a reference by computing the following five measures defined in \cite{salamon:MelodyReview:IEEESPM13}, first used in the MIREX 2005 Automatic Melody Extraction evaluation \cite{polinerMelodyEval}:
\textbf{Voicing Recall Rate} which computes the proportion of frames labeled as melody frames in the reference that are estimated as melody frames by the algorithm;
\textbf{Voicing False Alarm Rate} which computes the proportion of frames labeled as non-melody in the reference that are mistakenly estimated as melody frames by the algorithm;
\textbf{Raw Pitch Accuracy} which computes the proportion of melody frames in the reference for which the frequency is considered correct (i.e.~within half a semitone of the reference frequency);
\textbf{Raw Chroma Accuracy} where the estimated and reference $f_0$ sequences are mapped onto a single octave before computing the raw pitch accuracy;
and the \textbf{Overall Accuracy}, which computes the proportion of all frames correctly estimated by the algorithm, including whether non-melody frames where labeled by the algorithm as non-melody.
Prior to computing the measure definitions provided above, both the estimate and reference sequences must be sampled on the same timebase.

\subsection{Onset Detection}

The goal of an onset detection algorithm is to automatically determine when notes are played in a piece of music.
As is also done in beat tracking and segment boundary detection, the primary method used to evaluate onset detectors is to first determine which estimated onsets are ``correct'', where correctness is defined as being within a small window of a reference onset \cite{bock2012evaluating}.
From this, \textbf{precision}, \textbf{recall}, and \textbf{F-measure} scores are computed.

\section{Comparison to Existing Implementations}
\label{sec:comparison}

In order to validate the design choices made in \mireval{}, it is useful to compare the scores it reports to those reported by an existing evaluation system.
Beyond pinpointing intentional differences in implementation, this process can also help find and fix bugs in either \mireval{} or the system it is being compared to.

For each task covered by \mireval{}, we obtained a collection of reference and estimated annotations and computed a score for each metric using \mireval{} and the evaluation system being compared to.
In order to facilitate comparison, we ensured that all parameters and pre-processing used by \mireval{} were equivalent to the reference system unless otherwise explicitly noted.
Then, for each reported score, we computed the relative change between the scores as their absolute difference divided by their mean, or
$$
\frac{|s_m - s_c|}{(s_m + s_c)/2}
$$
where $s_m$ is the score reported by \mireval{} and $s_c$ is the score being compared to.
We then computed the average relative change across all examples in the obtained dataset for each score.

For the beat detection, chord estimation, structural segmentation, and onset detection tasks, MIREX releases the the output of submitted algorithms, the ground truth annotations, and the reported score for each example in each data set.
We therefore can directly compare \mireval{} to MIREX for these tasks by collecting all reference and estimated annotations, computing each metric for each example, and comparing the result to the score reported by MIREX.
We chose to compare against the results reported in MIREX 2013 for all tasks.

In contrast to the tasks above, MIREX does not release ground truth annotations or algorithm output for the melody extraction and pattern discovery tasks.
As a result, we compared \mireval{}'s output on smaller development datasets for these tasks.
For melody extraction, the ADC2004 dataset used by MIREX is publicly available.
We performed melody extraction using the ``SG2'' algorithm evaluated in 2011 \cite{salamon:MelodyExraction:TASLP:12} and compared \mireval{}'s reported scores to those of MIREX.
For pattern discovery, we used the development dataset released by Collins \cite{Collins2013} and used the algorithms submitted by Nieto and Farbood \cite{nieto2013discovery} for MIREX 2013 to produce estimated patterns.
We evaluated the estimated patterns using the MATLAB code released by Collins \cite{Collins2013}.
The number of algorithms, examples, and total number of scores for all tasks are summarized in Table \ref{tab:nexamples}.

\begin{table}[htb]
  \centering
\begin{tabularx}{\columnwidth}{l r r r}
\toprule
Task & Algorithms & Examples & Scores\\
\midrule
Beat Detection & 20 & 679 & 13580\\
Segmentation & 8 & 1397 & 11176 \\
Onset Detection & 11 & 85 & 935 \\
Chord Estimation & 12 & 217 & 2604 \\
Melody & 1 & 20 & 20 \\
Pattern Discovery & 4 & 5 & 20 \\
\bottomrule
\end{tabularx}
 \caption{Number of scores collected for each task for comparison against \mireval{}.}
 \label{tab:nexamples}
\end{table}

The resulting average relative change for each metric is presented in Table \ref{tab:comparison}.
The average relative change for all of the pattern discovery metrics was 0, so they are not included in this table.
For many of the other metrics, the average relative change was less than a few tenths of a percent, indicating that \mireval{} is equivalent up to rounding/precision errors.
In the following sections, we enumerate the known implementation differences which account for the larger average relative changes.

\begin{table*}[htb]
  \centering
\begin{tabularx}{\linewidth}{c *{8}{>{\centering\arraybackslash}X}}
\toprule
 \multicolumn{9}{ c }{Beat Detection} \\
  \cmidrule(lr){1-9}
F-measure  & Cemgil     & Goto       &  P-score   & CMLc       & CMLt       & AMLc       & AMLt       & In. Gain   \\
   0.703\% &    0.035\% &    0.054\% &    0.877\% &    0.161\% &    0.143\% &    0.137\% &    0.139\% &    9.174\% \\
  \midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{c c c c c *{4}{>{\centering\arraybackslash}X}}
 \multicolumn{9}{ c }{Structural Segmentation} \\
  \cmidrule(lr){1-9}
  NCE-Over   & NCE-under  & Pairwise F & Pairwise P & Pairwise R & Rand       & F@.5       & P@.5       & R@.5  \\
3.182\% &   11.082\% &    0.937\% &    0.942\% &    0.785\% &    0.291\% &    0.429\% &    0.088\% &    1.021\%  \\
  \midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{*{8}{>{\centering\arraybackslash}X}}
 \multicolumn{5}{ c }{Structural Segmentation (continued)} & \multicolumn{3}{ c }{Onset Detection}\\
  \cmidrule(lr){1-5}
  \cmidrule(lr){6-8}
  F@3 &   P@3        & R@3 & Ref-est dev. & Est-ref dev. & F-measure  & Precision  & Recall     \\
 0.393\%  & 0.094\% &    0.954\% & 0.935\% &    0.000\% &  0.165\% &    0.165\% &    0.165\% \\
  \midrule
\end{tabularx}
\begin{tabularx}{\linewidth}{*{2}{>{\centering\arraybackslash}X} c *{1}{>{\centering\arraybackslash}X} c *{1}{>{\centering\arraybackslash}X} c *{1}{>{\centering\arraybackslash}X} c c}
 \multicolumn{5}{ c }{Chord Estimation} & \multicolumn{5}{ c }{Melody Extraction}\\
  \cmidrule(lr){1-5}
  \cmidrule(lr){6-10}
  Root       & Maj/min       & Maj/min + Inv & 7ths & 7ths + Inv & Overall & Raw pitch & Chroma & Voicing R & Voicing FA \\
 0.007\% & 0.163\% & 1.005\% & 0.483\% & 0.899\% & 0.070\% &    0.087\% &    0.114\% &    0.000\% &   10.095\% \\
  \bottomrule
\end{tabularx}
 \caption{Average relative change for every metric in \mireval{} when compared to a pre-existing implementation.}
\label{tab:comparison}
\end{table*}

\subsection{Non-greedy matching of events}

In the computation of the F-measure, precision and recall metrics for the beat tracking, boundary detection, and onset detection tasks, an estimated event is considered correct (a ``hit'') if it falls within a small window of a reference event.
No estimated event is counted as a hit for more than one reference event, and vice versa.
In MIREX, this assignment is done in a greedy fashion, however in \mireval{} we use an optimal matching strategy.
This is accomplished by computing a maximum bipartite matching between the estimated events and the reference events (subject to the window constraint) using the Hopcroft-Karp algorithm~\cite{hopcroft1973n}.
This explains the observed discrepancy between \mireval{} and MIREX for each of these metrics.
In all cases where the metric differs, \mireval{} reports a higher score, indicating that the greedy matching strategy was suboptimal.

\subsection{McKinney's P-score}

When computing McKinney's P-score \cite{davies2009evaluation}, the beat sequences are first converted to impulse trains sampled at a 10 millisecond resolution.
Because this sampling involves quantizing the beat times to a 10ms grid, shifting both beat sequences by a constant offset can result in substantial changes in the P-score.
As a result, In \mireval{}, we normalize the beat sequences by subtracting from each reference and estimated beat location the minimum beat location in either series.
In this way, the smallest beat in the estimated and reference beat sequences is always $0$ and the metric remains the same even when both beat sequences have a constant offset applied.
This is not done in MIREX (which uses the Beat Evaluation Toolbox \cite{davies2009evaluation}), and as a result, we observe a considerable average relative change for the P-score metric.

\subsection{Information Gain}

The Information Gain metric \cite{davies2009evaluation} involves the computation a histogram of the per-beat errors.
The Beat Evaluation Toolbox (and therefore MIREX) uses a non-uniform histogram binning where the first, second and last bins are smaller than the rest of the bins while \mireval{} uses a standard uniformly-binned histogram.
As a result, the Information Gain score reported by \mireval{} differs substantially from that reported by MIREX.

\subsection{Segment Boundary Deviation}

When computing the median of the absolute time differences for the boundary deviation metrics, there are often an even number of reference or estimated segment boundaries, resulting in an even number of differences to compute the median over.
In this case, there is no ``middle'' element to choose as the median.
\mireval{} follows the typical convention of computing the mean of the two middle elements in lieu of the median for even-length sequences, while MIREX chooses the larger of the two middle elements.
This accounts for the discrepancy in the reference-to-estimated and estimated-to-reference boundary deviation metrics.

\subsection{Interval Sampling for Structure Metrics}

When computing the structure annotation metrics (pairwise precision, recall, and F-measure, Rand index, and normalized conditional entropy over- and under-segmentation scores), the reference and estimated labels must be sampled to a common timebase.
In MIREX, a fixed sampling grid is used for the Rand index and pairwise classification metrics, but a different sampling rate is used for each, while a fixed number of samples is used for the conditional entropy scores.
In \mireval{}, the same fixed sampling rate of 100 milliseconds is used for all structure annotation metrics, as specified in \cite{willis2013mirex}.

Furthermore, the start and end time over which the intervals are sampled depends on both the reference and estimated intervals while \mireval{}, always samples with respect to the reference to ensure fair comparison across multiple estimates.
This additionally requires that estimated intervals are adjusted to span the exact duration specified by the reference intervals.
This is done by adding synthetic intervals when the estimated intervals do not span the reference intervals or otherwise trimming estimated intervals.
These differences account for the average relative changes for the structure annotation metrics.

\subsection{Segment Normalized Conditional Entropy Scores}

When adding intervals to the estimated annotation as described above, \mireval{} ensures that the labels do not conflict with existing labels.
This has the effect of changing the normalization constant in the normalized conditional entropy scores.
Furthermore, when there's only one label, the normalized conditional entropy scores are not well defined.
MIREX assigns a score of 1 in this case; in \mireval{} we assign a score of 0.
This results in a larger average change for these two metrics.

\subsection{Melody Voicing False Alarm Rate}

When a reference melody annotation contains no unvoiced frames, the voicing false alarm rate is not well defined.
MIREX assigns a score of 1 in this case, while \mireval{} assigns a score of 0 because, intuitively, no reference unvoiced frames could be estimated, so no false alarms should be reported.
In the data set over which the average relative change for the melody metrics was computed, one reference annotation contained no unvoiced frames.
This discrepancy caused a large inflation of the average relative change reported for the voicing false alarm rate due to the small number of examples in our dataset.

\subsection{Weighted Chord Symbol Recall}
The non-negligible average relative changes seen in the chord metrics are caused by two main sources of ambiguity.
%Some interval mappings aren't well-defined.
First, we find some chord labels in the MIREX reference annotations lack well-defined, i.e. singular, mappings into a comparison space. 
One such example is $D:maj(*1)/\#1$.
While the quality shorthand indicates ``major'', the asterisk implies the root is omitted and thus it is unclear whether $D:maj(*1)/\#1==D:maj1$.
%Some chords get dropped in different scenarios, and we can't replicate the logic. 
Second, and more importantly, such chords are likely ignored during evaluation, and we are unable to replicate the exact exclusion logic used by MIREX. 
This has proven to be particularly difficult in the two inversion rules, and manifests in Table \ref{tab:comparison}. 
For example, it seems as though $Bb:maj(9)/9$ was \emph{not} excluded from the MIREX evaluation, contradicting the description provided online.
This chord alone causes an observable difference between the results of our implementation and previously reported results.

\section{Towards Transparency and Community Involvement}
\label{sec:discussion}

The results in Section \ref{sec:comparison} clearly demonstrate that differences in implementation can lead to substantial differences in reported scores.
This underlines the need for transparency and community involvement in comparative evaluation.
The primary motivation behind developing \mireval{} is to establish an open-source, publicly refined implementation of the most common MIR metrics.
By encouraging MIR researchers to use the same easily understandable evaluation codebase, we can ensure that different systems are being compared fairly.

While we have given thorough consideration to the design choices made in \mireval{}, we recognize that standards may change over time and that new metrics may be proposed.
Towards this end, \mireval{} is hosted on Github\footnote{URL redacted}, which provides a straightforward way of proposing changes to the codebase using the Pull Request feature.
With active community participation, we believe that \mireval{} can ensure that MIR research converges on a standard methodology for evaluation.

\bibliography{mir_eval}

\end{document}
